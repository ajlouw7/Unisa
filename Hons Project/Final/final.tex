\documentclass[12pt,a4paper]{article}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage[a4paper,margin=0.8in]{geometry} 
\usepackage{natbib}
\usepackage{longtable}
\usepackage[pdftex]{graphicx}
\usepackage{appendix}
\usepackage{subfig}

\bibliographystyle{agsm}

%opening
\title{An investigation into use of machine learning techniques as a tool in the study of the factors that affect life expectancy on a global scale}
\author{Adriaan Louw (53031377)}

\begin{document}

\maketitle

\newpage

\thispagestyle{plain}
\begin{center}
    \Large
    \textbf{Abstract}
\end{center}



The pursuit of increasing ones life expectancy is a very human endeavour. In order to study human life expectancy, demographers and other scientists, study the factors that affect life expectancy. Those include socio-economic indicators like income, educational attainment and per capita spending on healthcare. Understanding these relationships can lead a country to better a understand of which socio-economic areas to focus on, in order to increase their life expectancy. A popular techinique when studying the relationship between life expectancy and these factors is linear regression. This study will use datasets, from the World Bank, that descibe these indicators on a global scale and compare how well linear regression fits the data in comparrison with various machine learning techniques. These techniques are k-nearest neighbour, Support Vector Machines and clustering. All of these techniques will then be compared using Mean Sqared Error.

\newpage


\tableofcontents

\listoffigures

\listoftables

\newpage

\section{Introduction}

Human beings have always had a fascination with longevity. Myths like the fountain of youth or the Holy Grail are a testament to this fact. Today, longevity and causes of mortality are studies by professionals like Demographers and Actuaries. Trying to determine why some people or group live long lives.    

This study investigates the use of Machine Learning techniques in studying determinants of life expectancy for countries. Indicators that have shown to have some form of correlation with life expectancy will be selected. Their relationship with life expectancy will be investigated using various techniques from Machine Learning and contrasted to various forms of regression whose use is ubiquitous in the literature. This analysis will seek out to prove the appropriateness of using these machine learning algorithms for use in research to find the exact correlation between these indicators and life expectancy. It is the hypothesis of this study that machine learning techniques like k-Nearest Neighbour and Support Vector Machines will model the indicator/life expectancy relationship better than regression techniques can. Also that these techniques can create more accurate models. In the hope that the causes of long life expectancies in certain countries can be better understood \citep{Chen2017}. This study does not aim to prove causation between the indicators chosen and life expectancy, but rather the usefullness of machine learning algorithms as  tools. 

Statistical regression techniques are predominantly used algorithm for data analysis. Linear regression (Section \ref{regression}) forinstance assumes a linear relationships between the independant variables and the dependant variable. Which might not be the case. This we will discuss in Literature Review (Section \ref{lit}).

\section{Literature Review} \label{lit}

Life expectancy and mortality are 2 related terms. Mortality is generally expressed as a mortality rate. It describes the rate at which people die under certain circumstances. Life expectancy (Section \ref{LEXP}) is the amount of years an individual or group of people are expected to live. If the amount of people who are dying increases, the mortality rate increases and the life expectancy for people in that group decreases and visa versa.

\subsection{What do we mean by life expectancy?} \label{LEXP}

A life table is a table given for a specific year that contains the probability that a person of a certain age will die in that specific year. Acuataries and Demographers use life tables in the insurance industry and the study of demographics respectively. Life expectancy is one element of a life table. Both countries and the United Nations create life tables for use in policy creation.

There are 2 types of life tables, namely period and cohort life tables. A cohort is a group of people who were born in the same year. A cohort life table will follow a cohort over its lifetime until every member of the cohort has died. A cohort life table requires the mortality information of the cohort over many years. This information is often unavailable. While for a period life table, a hypothetical cohort is created and subjected to current mortality rates. This gives the user of the period timetable a window to see mortality rates at that point in time. This makes period life tables the most common type \citep{Arias2017}. 

A life expectancy figure from a period life table is called a period life expectancy.

\subsection{Multi indicator studies}

In this section, studies that used various indicators will be discussed.

\cite{Kabir2008} investigated how well the life expectancy of 93 developing countries were predicted by indicators like income,education and fertility (among others). It classied a countries life expectancy into 3 categories. Then used a probit model where the input variables have a linear relationship. Multiple Ordinary Least Squares Regression was then applied to study inicators' influences.

The study \cite{Hu2015}, also used a linear regression model of GDP per capita, Gini indeces, ect with respect to life expectancy. The intention of the study was to link income inequality to mortality rates and life expectancy.

While \cite{Shaw2005} investigated factors like smoking, pharmasutical spending, amount of butter consumed and amount of fruit and vegetables consumed. Then putting them in a linear model and applying regression.   

In an examination of 108 countries, \cite{Hassan2016} investigated indicators like education, GDP and health spending as it relates to life expectancy. They used Grossmans model \citep{Grossman2000}  
to model their data and Vector Error Correction Model to anlyse the data.

By using the componets of the Human Development index and Pearsons r with multivariate regression, \cite{Bulled2010} investigated the link between life expectancy, education and reproduction.

This study aims to add machiine learning algorithms as a tool to life expectancy studies, like these, that look at multiple factors that affect life expectancy.





%\subsection{Multi variate studies}

\subsection{Determinants of life expectancy}

This section summarises the relationship some indicators have with life expectancy.

\subsubsection{Income} \label{income}

The relationship between life expectancy and income has been given a lot of attention in academic circles \citep{Preston1975, Hu2015, Chetty2016, Oeppen2019}. 

\cite{Preston1975} was the first to show the correlation between life expectancy and per capita income. His original curve can be seen in Figure \ref{Preston}. As we can see from Figure \ref{Preston}, for low income countries, life expectancy increases rapidly with per capita income. Whereas in high income countries a small increase in per capita income does not have a large effect on life expectancy.

This relationship has also been shown in more recent studies \citep{Chetty2016,Oeppen2019}. Even though \cite{Shkol2019} found that in Russia, the Preston curve is not an accurate predictor of life expectancy, they found that the actual life expectancy should be ``substantially higher'' when comparing to the Preston curve predicted value. 

Studies in first world countries involving mortality rather that life expectancy have also found a relationship with income level \citep{Blakely2004,Kalwij2013,VonGaudecker2007}.

Just 16\% of the improvement in life expectancy between the 1930s and 1960s could be explained by rising income levels \cite{Preston1975}. Which seems to indicate that a countrie's life expectancy is dependant on more than income levels.  

\begin{figure}
\centering
\includegraphics[height=9cm, width=13cm]{The-original-Preston-Curve-1975.jpg}
\caption{The original Preston curve from \cite{Preston1975}}
\label{Preston}
\end{figure}

\subsubsection{Education attainment}

\cite{Kaplan2015} investigated the relationship between educational attainment and life expectancy in eight states in the United States. They found that even when controlling for variables like income, race, sex, and common medical issues like cardiovascular disease, the relationship between educational antainment and life expectance remains statistically significant.

\cite{Luy2019} studied 3 developed nations, namely the United States, Italy and Denmark. They have also found a strong correlation between education levels and longevity.

But what is the nature of this correlation? According to \cite{Deary2004}, Intelligence Quotient or IQ could explain the association. While \cite{Hayward2015} does not believe in a ``causal relationship'', but rather that it depends on factors like ``time, place, and the social environment''.

In an attempt to find a causal relationship between education and life expectancy, \cite{VanKippersluis2009} investigated the result of the Netherlands increasing the mandatory number of years a child had to attend school for to 7 years. It was 6 years previously. \cite{VanKippersluis2009} found a decrease in mortality of 3\% for 81-year old males who had the additional year of schooling. 

This relationship appears strongest in more developed countries where the life expectancy is already above 60 years \citep{Bulled2010}. In these countries, any educational investment leads to greater compensation for the learner than they would get in a less developed country \cite{Bulled2010,Handwerker1986}. In addition, \cite{Kabir2008} also studied this relationship, among others, with regards to developing countries and did not find a correlation. 

The literature appears not to be in agreement.

The question remains, which educational indicators should be used when investigating the relationship between education and life expectancy?

Various educational indicators have been used in the literature for comparing to life expectancy. One approach is to use the International Classification of Education (ISCED) system \citep{UNESCO2012}. The ISCED 2011 standard consists of 9 levels ranging from ISCED level 0 (Early childhood education) to ISCED level 8 (Doctoral or equivalent level).

\cite{Luy2019} used the United Nations ISCED-97 (consisting of 7 levels) scale to break education attainment down into 3 levels namely Low (None to Lower Secondary), Medium (Upper secondary) and High (Tertiary education). In \cite{VanKippersluis2009} the Dutch SOI system (Standaard Onderwijs Indeling). Which, according to \cite{VanKippersluis2009}, is similar to the ISCED system. While in \cite{Deboosere2009}, educational attainment was broken into 5 levels, also ranging from no education to Tertiarty education. 

\cite{Kaplan2015} broke educational attainment into 4 levels ranging from less than high school to college graduate.

In the study \cite{Bulled2010}, the relationship between educational investments and fertility against life expectancy, over 193 countries, was investigated. They used adult literacy and the enrolment ratios for primary,secondary and tertiary schooling.


%For more see \cite{Montez2015} Much information!!!!
%\cite{VanBaal2016a}

\subsubsection{Spending on health}

Healthcare spending and life expectancy in the Unites States, between 1960 and 2000, was compared in  \cite{Cutler2006}. They found that increased spending on health per capita, controlling for inflation, is positively correlated to US life expectancy for the time period in question. 

Most Eastern European countries, who have joined the European Union, have seen an increase in healthcare spending. This has generally been accompanied by an improvement in life expectancy \citep{Jakovljevic2016}. This has to be seen in the context of the so called ``Russian Mortality Crisis'' where former Soviet Union countries faced a sudden drop in life expectancy after the fall of the Berlin wall \citep{Brainerd2005}. \cite{Jakovljevic2016} found that the best metric to use when comparing health spending of countries, is to use their total per capita health spending (in US dollars).

The same relationship was found in Canada. When spending on healthcare is decreased, life expectancy follows \citep{cre}.

It is well known that life expectancy in Sub-Saharan Africa is low. Here spending on health care can also be correlated to increases in life expectancy. Even though poor governance can undo some of the effects of increased spending \citep{Makuta2015}.

A country's per capita healthcare is not necessarily in proportion to its per capita income. In 2005, the United States spent 50\% more on healthcare per capita than its income per capita would suggest \citep{Anderson2008}.



%Husain, AR (2002), “Life Expectancy in Developing Countries: A Cross-Section Analysis”, The Bangladesh Development Studies, 28(1&2)








%\cite{Shaw2005} showed that pharmaceutical expenditures shows a positive correlation with life expectancy in OECD countries.

%medical spending \cite{Cutler2006}

%?Grossman?
%2017 determinants of health: an economic perspective  ????
%1972 The Demand for Health: A Theoretical and Empirical Investigation,

%\cite{Grossman2000}

\subsubsection{Unemployment}

According to \cite{Bonamore2015}, the literature has 2 main views on the relationship between unemployment and life expectancy. The first view states that during an economic downturn, people suffer from more stress and depression. This leads to more unhealthy lifestyle choices like smoking and alcohol. Which in turn lowers life expectancy. \cite{Bonamore2015} cites the works of \cite{Lundin2014,Montgomery2013,GARCY20121911,BROWNING2012599,valos2012,backhans2011unemployment,DEB2011317} and \cite{Strully2009}, who take this view. The second view focusses on times when there is economic growth, i.e. less unemployment. This period of economic growth also can lead to stress eg. burning out and having less time for activities that benefit ones health. Like going to the gym. This view is held by \cite{TapiaGranados2011,RUHM2005341,TapiaGranados2005,NEUMAYER20041037} and \cite{Ruhm2000}. Then there are also studies express the view that no connection can be established \citep{Bonamore2015}. The view of  \cite{Bonamore2015} is that this relationship is non-linear.


%unemployment \cite{Bonamore2015} \cite{Roelfs2011} \cite{Roelfs2015} 

\subsection{Modelling Techniques} \label{moddelingTechniques}

\subsubsection{Linear Regression} \label{regression}
Linear Regression is a popular technique, used to find relationships in data. As the name suggests Linear Regression assumes a linear relationship between the input variables and the result \citep{Murphy}. This might not be the case for the target function. The target function could be any potential function. In the case of life expectancy modelling, we know that according to the Preston curve (Section \ref{income}), the relationship between income and life expectancy is not linear. Thus using Linear Regression should return a sub-optimal result unless the data is transformed using some non-linear function into a linear space. Linear regression was used in the following studies: \cite{Chetty2016,Jakovljevic2016,Hu2015,Mackenbach2013,Bulled2010}. 

The Ordinary Least Square (OLS) method of estimation will be used. This algorithm will be implemented in Python and used to analyse how well this technique models the problem.

The Ordinary Least Squares method can be described as:

\begin{equation}
 y_i = x^T_i \beta + c
\end{equation}

and

\begin{equation}
\beta = (X^T X)^{-1}X^Ty
\end{equation}


where $x_i$ is the ith data point, $X$ is all the data points in matrix form and $y_i$ is the predicted life expectancy.

\subsubsection{Logistic regression}
This regression is appropriate when the dependant variable is discrete, e.g. a yes/no answer. It also assumes a linear relationship between inputs. In contrast to linear regression, this linear sum is passed through the sigmoid function \citep{Murphy}. All of this makes it inappropriate for non-linear target functions and this study.

\subsubsection{k-Nearest Neighbour}

The k-Nearest Neighbour algorithm (kNN) is an instance based form of machine learning. It uses the classification of those datapoints closest to the data point to be classified to determine its classification. The kNN-algorithm allows for non-linear problem spaces to be classified, because it does not make an assumption on the nature of the problem space. It just sees a data point as a function of its closes neighbours. This is usefull for indicators like income that are highly non-linear as described in Section \ref{income}. Additionally, how the algorithm determined its output value is transparent and can be used to study how various components affect the end result. This is important , because if the algorithm was not transparent it can not be seen as a replacement for other algorithms that are transparent like regression alorithms. In this study, the standard kNN-algoritm will be altered to accomodate a real valued output and not just a class classification. This will be accomplished by taking the mean life expectancy for all the data points determined to be closest to the target point. Care will have to be taken to reduce the number of features of the data, because this algorithm is sensitive to the so-called ``curse of dimentionality'' \citep{Mitchell}. As discussed in Section \ref{edudata}, educational attainment will comprise of many indicators. This could lead to educational attainment being weighted more than other indicators. Thus each educational indicator will be given a fractional weight. If there are 4 educational attainment then each indicator will have a 4th of the weight of other indicators. 

The value of $k$ will have to be found experimentally and will be affected by the density of the data points. One potential issue could be if the data tends to cluster around certain points. This could happen if developing countries cluster together and developed countries cluster together. That could make developing countries, that are on the higher end of the development scale, more difficult to classify, since not a lot of data points will be very close to them. 

\subsubsection{Support Vector Machines}

The classic Support Vector Machine (SVM) is used in classification tasks. It involves determining the decision surface with regards to the data points closest to the surface. This study will use a modified SVM algorithm that makes it useful for regression tasks. This is called Support Vector Regression (SVR). The SVR algorithm is described in \cite{smola2004}. The final model can be described as:

\begin{equation}
 f(x) = \sum_{i=1}^l (\alpha_i-\alpha_i^*)k(x_i,x)+b
\end{equation}

where $\alpha_i$ and $\alpha_i^*$ are Lagrange multipliers, $b$ is the bias and the term $k(x_i,x)$ is the kernel function. The kernel is a function to determine the similarity between 2 input vectors. Kernel functions are used to map a non-linear problem space into a smaller subspace where the features are linearly seperable. A kernel has to map from the dimension of the problem space down to 1 dimension. Instead if computing the dot product of 2 datapoints, we can replace it with a kernel that requires less computation. Various kernels will be investigated, including the Radial-basis function

\begin{equation}
 k(x_i,x) = exp(-\frac{\mid\mid x-x_i \mid\mid^2}{2\sigma^2})
\end{equation}

and the polynomial kernel

\begin{equation}
 k(x_i,x) = (x_i.x +1)^p
\end{equation}

Unfortunately, SVR models are difficult to analyse. Making it hard to determine how the input features of the model affect the output. In order for SVR to be seen as a viable alternative to multivariate regression, an appropriate technique has to be found that can assist in this dilemma. For purpose, this study will use the techniques described in \cite{USTUN2007299} to understand which input feature the SVR model deems most important for the modelling of our life expectancy problem space.

In the algorithm described in \cite{USTUN2007299}, a kernel matrix or Gram matrix has to be computed. This matrix $K$ will be of size $N X N$ and contains in each cell the kernel function corresponding to the input vectors of the same index such that

\begin{equation}
 K_{ij} = k(x_i,x_j)
\end{equation}

The vector of input data(I) is of size $M X N$ where $M$ is the number of input features and $N$ is the size of the dataset. The Correlation matrics(R) is calculated by correlating each row of I with each column in K. This gives a resultant vector R of size $N X M$. Now we have a measure of the impotance of each input to the kernel matrix. This resultant matrix is then converted to an image so that the relationships can be seen visually.

\subsubsection{Clustering} \label{clustering}

Clustering techniques are a form of unsupervised learning where the algorithm tries to group the data in a number of groups. In essence, classifying the data into categories. This cannot be easily compared to regression techniques. This study will rather follow the approach of Clustered linear regression(CLR) \citep{Ari2002}. In CLR, clustering techniques are used to break the dataset into clusters. Then apply linear regression to the datapoints in each cluster. In this way one linear regression model does not have to account for all data the data in the data set. Each regression model can be tailored to data similar to it, thereby increasing the chance of getting a good linear fit on the data. This technique also allows the researcher to gain information about the dataset. If the data is very clustered, it could mean that the promlem space in not very linear and that data tends to ``jump'' between clusters. Comparisons will be made to how the number of clusters affect the results from the regression analyses.

Various clustering algorithms exist. They can be classified into 2 broad groups. The first group is flat clustering. In this group, the algorithms break the data into various groups that have no hierarchy. In other words, there are no group of groups. The second type in hierarchical clustering \citep{Murphy}. 

Agglomerative clustering and divisive clustering are the 2 types of hierarchical clustering. Agglomerative clustering starts of withm each data point being a cluster and then combining clusters until some condition is reached. Divisive clustering adds all the datapoints to one cluster and then breaks them into smaller sub-clusters until some condition is met \citep{Murphy}.

Each cluster has an initial start point, i.e. the initial position of its centroid. The centroid will be randomly placed in the hypothesis space. Each data point is the assigned to the nearest centroid, determined by Euclidean distance. The centroids position will then by updated to be the mean of all its newly assigned datapoints. The algorithms then repeatedly updated the position of each centroid based on the closest data points, until the centroid stop moving. It is worth noting that the algorithm might not coverge to the optimum solution. Therefor multiple runs of this algorithm will be necessary \citep{Murphy}.



\subsubsection{Neural Networks}
Even though Neural networks are capable of representing non-linear hypothesis spaces \citep{Mitchell}, they are not appropriate for this study for a couple of reasons. Firstly, the amount of processing power and processing time required, will not be available to this study. Secondly, the results of neural networks are hard to interpret. How the Neural Network came to its conclusion is not clear to the researcher. Which makes it unsuitable as a tool to study the relationship between life expectancy and its various indicators.

\subsubsection{Decision Trees}

Traceability and understandability are some of the hallmarks of Decision Trees. These algorithms are suited problem spaces where the target function and the input attributes are discrete values. It is possible to approximate continuous input attributes by making a branch in the tree when a value is smaller or greater than some value, or is between some value. For functions where input attributes span over large ranges, this leads to very large and sub-optimum trees \citep{Mitchell}. Many decision tree algorithms exist, like ID3 and C4.5. The problem of determining life expectancy from socio-economic indices has a continuous target function output and continuous input attributes. Therefor, Decision Trees will be excluded from this study.


\subsection{Datasets}

\subsubsection{Life Expectancy}
For life expectancy data, this study will use the indicator ``Life expectancy at birth, total(years) \{SP.DYN.LE00.IN\}'' from the World Bank's Development Indicators Database \citep{WDI}. This is a weighted average combining both male and female life expectancy and is calculated in a period life table (see Section \ref{LEXP}). Only the data that is available for the last 30 years (1981-2010) will be used. This is in keeping with other studies where the amount of years that their studies look back on is limited to relatively recently \citep{Luy2019, Hu2015,Tarkiainen2012,Kabir2008, Shaw2005}. 

\subsubsection{Income}

This study will use GDP per capita as was used in \cite{Oeppen2019,Shkol2019,Mackenbach2013} and \cite{DeVogli2005}. The source will be GDP per capita, PPP (constant 2011 international \$)\{NY.GDP.PCAP.PP.KD\} from the World Bank \citep{WorldBank_gdp}.

\subsubsection{Educational Attainment} \label{edudata}

This study will use the same indicators and was used in \cite{Bulled2010}. For an adult literacy indicator, SE.ADT.LITR.ZS will be used from the world bank website \citep{adultLiteracy}. This indicator describes the percentage of adults, from the age of 15, who can read and write to a certain level of proficiency. Determining the literacy level of a country is a difficult endeavour. The definition of literacy might be exactly the same between countries, because this indicator uses a lot of data that each country determines on its own. This also might include some measure of competancy with numbers.

The indicator SE.PRM.ENRR will serve as the enrollment ratio for primary education \citep{worldbankenrollmentprimary}. As the name suggests, it is the pecentage of students in primary school. It is calculated as the fraction of students in primary school over the amount of students that should be in prmary school basaed on population figures. This can lead to a value greater than 100\%. This is due to older students or adults that should have completed primary school that are now undergoing primary school education. This can be a sign of a poorly performing school system. This indicator also does not take issues like truancy into account. 

For secondary school enrolment, SE.SEC.ENRR  \citep{worldbankenrollmentsecondary} will be used. This   indicator can be higer than expected or above 100\% for the same reasons as the primary school enrollment indicator. 

SE.TER.ENRR \citep{worldbankenrollmenttertiary} will be used for tertiary enrollment. A high number here tends to indicate that it is a number from an developed country, because tertiary education is much more of a luxury in developing countries.

It is expected that the enrollment at tertiary and secondary will be highly correlated to industrialised countries and thus with life expectancy. The world bank credits \cite{UNESCOInstituteofStatistics2019} for all of theses educational indicator data.

\subsubsection{Per Capita spending on health}
To represent Per capita spending on health in this study's model, SH.XPD.CHEX.PP.CD \citep{pphealthspending} which describes Current health expenditure per capita, will be used.

\subsubsection{Unemployment}
Unemployment will be ignored for this study because sufficient data on unemployment per country is not available. This can be seen by looking at indictors like SL.UEM.TOTL.NE.ZS \citep{unemploymentdata} which describe percentage unemployment per country for both sexes.



















\section{Methodology/Procedure}


This study will attempt to create a model that can predict life expectancy for a country based on various socio-economic conditions in the country over a 30-year period. This will be done by implementing the various algorithms discussed in Section \ref{moddelingTechniques} in Python. Then these programs will be applied to the dataset. Python is a common tool used in machine learning applications, with a wide variety of libraries that can be used. The algorithms will be newly implemented for this project but libraries like Numpy will be used to help with reading data from files and writing data to files.

Then by using thise programs we will evaluate how well these machine learning techniques model the relationship between Life expectancy and the chosen indicators. Unlike \cite{Shaw2005}, this study will not take into account the age distribution of each country.

The philosophical standpoint of this study is Positivism. By using the scientific method, this study will comprise of an experiment to inductively determine whether machine learning techniques can provide more accurate life expectancy models than those created using regression. This cross-sectional study will use life expectancy indicators shown, from the literature, to have some correlation to life expectancy.

Firsly appropriate data sets will be chosen. Taking into account that some indicators might have sparse information. This will impact the size of the dataset. Looking at available indicators like \cite{adultLiteracy}, we can see there are approximately 200 countries. If all the indicators have data for each year in the study there will be around 6000 data points. Looking at indicators like \cite{adultLiteracy} and \cite{pphealthspending}, it is clear that the data is quite sparse, especially for developing countries. The intention is to have the study encompass developed and developing countries, but this will depend on data availability.

Once the dataset is finalised, the algorithms and their permutations described in Section \ref{algo} will be applied. Three models will be created for each algorithm and permutation thereof. The first model will be using all the data. The second will only use data from developed countries and the third will only use data from developing countries, data permitting. Cross-validation will be applied to the created models as described in Section \ref{cross}. The k data subsets will be determined before starting the validation. Each algorithm will  then be run on the same data sets to facilitate comparison.

The mean squared error will be calculated for each of the k Cross-Validation datasets for each of the algorithms. The average of each of these mean squared errrs will be calculated for each algorithm.This number will be used to determine the fitness of each algorithm and enables comparison. The mean squared error was chosen as the method of comparison due to its ease of use and that it can be applied to all the algorithms.

\subsection{Choice of algorithms} \label{algo}

As discussed in Section \ref{moddelingTechniques} of the Literature Review, this study will compare k-Nearest Neighbour, Support Vector Machines and Clustered linear regression with Linear regression. Linear Regression will be used as a baseline for comparison on the dataset. 

\subsection{Cross-Validation} \label{cross}

By using stratified \textit{k}-fold cross validation, this study will aim to reduce the impact of the relative small dataset that will be analysed. This form of cross validation will ensure that when the validation set is chosen, no important data points are ignored for training. The data will be broken down randomly into \textit{k} subsets of equal size. Each data subset will also contain equal amounts of data points with low and high life expectancies, so that no dataset is completely towards one end of the data range. One data subset is chosen to be the validation subset and the remaining $\textit{k} - 1$ subsets are combined into the training set. The model is then trained on the training dataset andt its performance is measured against the validation subset. This is done \textit{k} times in order for each subset to be the validation subset. For each of the training runs the mean of the error will be calculated \citep{Mitchell,Murphy}. The value of \textit{k} will be dependant on the final dataset. 







\section{Results}
\begin{figure}
\centering
\includegraphics[height=16cm, width=16cm]{matrixScatterPlot.png}
\caption{Matrix scatter plot of data}
\label{scatter}
\end{figure}

The data set for each indicator was downloaded seperately. The data was then combined based on country and year. Sparse data became a problem. If at least one indicator did not have data for a specific year and country, then the entry for that that year and country was removed from the dataset. The indicator for Adult Literacy was also removed from all entries due to that indicator having very sparce data. The total number of accepted data points were 1062. For each of those data points, every indicator (except adult literacy) was defined. 

The data for each indicatr was normalised between 0 and 1. For each indicator, the largest value was assigned 1 and the smallest 0. The rest was then scaled proportionally between 0 and 1. This was done so that algorithms like kNN do not favour one indicator/dimention over another due to its relative size.

The values for Primary-, Secondary-, and Tertiary Enrollment was added together and divided by 3 to create a new Indicator namely ``Education''. This was done in order to reduce the dimentionality of this 7-dimentional data set to a more manageble 5-dimentional data set. The dataset can be visually seen in a matrix scatter plot in Figure \ref{scatter}. It is worth noting that the Preston curve can be seen in this catter plot where Life Expectancy and GDP Per Capita intersect.

The following procedure was followed in order to get 10 stratified folds. The dataset was then sorted according to decending life expectancy. The dataset was then split in half such that the life expectancy of each data point in the ``top'' data set is higher than the life expectancy of each data point in the ``bottom'' data set. Then 10 folds of equal size was created from the ``top'' and ``bottom'' data sets, by taking 53 data points from each set. Thus each fold has 106 data points, with 53 high life expectancy points and 53 low life expectancy points. 

This process was done 10 times to create 10 runs of 10 folds each. Each dataset contains the same data but the data is spread around the different folds randomly.

\subsection{Linear Regression Results}

As described in Section \ref{regression} the OLS algorihm was implemented in Python and run for each fold each dataset. The results can be see in Table \ref{OLSResults}. The table shows the Average Mean Error per dataset. This is done by calculating the mean error of each fold in a dataset and then computing the average over all the folds in the dataset. The same process was followed for the Standard Deviation. The average Error over all the runs and thus all the runs of the algorithm was 9.609 years. While the Standard Deviation was 7.619. 

\begin{table}
\begin{tabular}{|c|c|c|}
\hline
Run & Average Mean Error & Average Standard Deviation of Error\\
\hline
1 & 9.598848355 & 7.594930051 \\
2 & 9.626031548 & 7.633113769 \\
3 & 9.609315926 & 7.621318362 \\
4 & 9.605118586 & 7.635851148 \\
5 & 9.606997134 & 7.65353075 \\
6 & 9.6332431 & 7.622023874 \\
7 & 9.590347552 & 7.621950019 \\
8 & 9.607112393 & 7.611794799 \\
9 & 9.598032704 & 7.608716795 \\
10 & 9.619144548 & 7.590407725 \\
\hline
Average: &9.609419185 & 7.619363729 \\
\hline
\end{tabular}
\caption{OLS Regression Results}\label{OLSResults}
\end{table}

\begin{figure}
\begin{tabular}{ccc}
\subfloat[Fold 1]{\includegraphics[width = 2in]{kNNErrorPlot1.png}} &
\subfloat[Fold 2]{\includegraphics[width = 2in]{kNNErrorPlot2.png}} &
\subfloat[Fold 3]{\includegraphics[width = 2in]{kNNErrorPlot3.png}} \\
\subfloat[Fold 4]{\includegraphics[width = 2in]{kNNErrorPlot4.png}} &
\subfloat[Fold 5]{\includegraphics[width = 2in]{kNNErrorPlot5.png}} &
\subfloat[Fold 6]{\includegraphics[width = 2in]{kNNErrorPlot6.png}} \\
\subfloat[Fold 7]{\includegraphics[width = 2in]{kNNErrorPlot7.png}} &
\subfloat[Fold 8]{\includegraphics[width = 2in]{kNNErrorPlot8.png}} &
\subfloat[Fold 9]{\includegraphics[width = 2in]{kNNErrorPlot9.png}} \\
\subfloat[Fold 10]{\includegraphics[width = 2in]{kNNErrorPlot10.png}} 
\end{tabular}
\caption{Mean Error in Predicted Life Expectancy per Fold for the unweighted kNN Algorithm for run 1}\label{kNNResultsPerFold}
\end{figure}

\subsection{k-Nearest Neighbour Results}


\begin{table}
\begin{tabular}{|c|c|c|c|}
\hline
Run & k & Average Mean Error & Average Standard Deviation of Error\\
\hline
1 & 41 & 2.120386929 & 2.389743605\\
2 & 35 & 2.118507644 & 2.330946813\\
3 & 39 & 2.120214137 & 2.343546292\\
4 & 39 & 2.122089962 & 2.345210974\\
5 & 39 & 2.122707998 & 2.339231525\\
6 & 39 & 2.122234215 & 2.336469553\\
7 & 39 & 2.120867119 & 2.339882294\\
8 & 39 & 2.11824741 & 2.340979544\\
9 & 39 & 2.119204169 & 2.339061524\\
10 & 39 & 2.118872887 & 2.338973832\\
\hline
Average: & 38.8& 2.120333247 & 2.344404596 \\
\hline
\end{tabular}
\caption{Unweighted kNN Results for best value of k}\label{kNNResultsUnweighted}
\end{table}

\begin{table}
\begin{tabular}{|c|c|c|c|}
\hline
Run & k & Average Mean Error & Average Standard Deviation of Error\\
\hline
1 & 41 & 2.121160816 & 2.381492154\\
2 & 28 & 2.110539349 & 2.262304954\\
3 & 39 & 2.11851791 & 2.314154022\\
4 & 41 & 2.127764586 & 2.358604755\\
5 & 40 & 2.124401498 & 2.315950553\\
6 & 41 & 2.119369173 & 2.327937177\\
7 & 38 & 2.112441118 & 2.344628653\\
8 & 39 & 2.101194006 & 2.343523783\\
9 & 41 & 2.125380836 & 2.322854298\\
10 & 37 & 2.091687809 & 2.275117546\\
\hline
Average: & 38.5& 2.11524571 & 2.324656789 \\
\hline
\end{tabular}
\caption{Weighted kNN Results for best value of k}\label{kNNResultsWeighted}
\end{table}

There were 2 versions of the kNN-algorithm. The first was an unweighted implementation where the predicted Life Expectancy was calculated as an average of the life expectancy of the k nearest neighbours. The second version was a weighted nearest neighbour algorithm. The predicted life expectancy($LE_p$) was calculated with the following formula:

\begin{equation}
 LE_p = \frac{\sum_i^k{LE_ie^{d_i}}}{\sum_i^k{e^{d_i}}}
\end{equation}

where $LE_i$ is the life expectancy of the ith data point and $d_i$ is the distance from the ith data point to the point in question. The results of the unweighted kNN-algorithm can be see in Table \ref{kNNResultsUnweighted} and the results for the weighted kNN-algorithm can be found in Table \ref{kNNResultsWeighted}. Figure \ref{kNNResultsPerFold} shows the results from the 1st run of the Unweighted kNN-algorithm per fold. 


% The kNN-algorithm was run for the values of k from 1 to 106 for each fold. The mean error was calculated for each fold and value of k. These values can be seen in Figure \ref{kNNResultsPerFold} where the mean error is graphed with respect to the value of k for each fold. In Figure \ref{kNNResultsAllFolds} we can see a plot of the mean error in life expectancy over all 10 folds versus k. The bands indicate the standard deviation of the data. 
% 
% The final results for the kNN-algorithm is in Table \ref{kNNResults}. Where we can see that the best performing value of k (in other words the value of k for which the mean error is the smallest) differs wildly between folds. The average being 30.4.

\begin{figure}
\centering
\includegraphics[height=16cm, width=16cm]{kNNErrorPlot.png}
\caption{kNN Error Data Plot with standard deviation over all folds}
\label{kNNResultsAllFolds}
\end{figure}


\subsection{Clustering Results}\label{Clustering Results}


\begin{table}
\begin{tabular}{|c|c|c|}
\hline
\# of centroids & Average Mean Error & Average Standard Deviation of Error\\
\hline
2 & 7.762052171 & 5.96294992\\
3 & 7.629710319 & 5.844772366\\
4 & 7.005638769 & 5.599632559\\
5 & 6.631308225 & 5.422260091\\
6 & 6.325310938 & 5.132051508\\
7 & 6.167973105 & 5.048466276\\
8 & 6.12395204 & 4.873890668\\
9 & 6.925555773 & 4.823754093\\
10 & 6.022997788 & 4.71522236\\
11 & 6.495814931 & 4.626752313\\
12 & 5.857179844 & 4.626444289\\
13 & 7.591622412 & 4.492052798\\
14 & 5.712254947 & 4.549610122\\
15 & 7.823888766 & 4.400055159\\
\hline
\end{tabular}
\caption{Clustering algorithm results per randomly selected number of centroids}\label{CentroidRandomResults}
\end{table}







\begin{table}
\begin{tabular}{|c|c|c|}
\hline
Run & Average Mean Error & Average Standard Deviation of Error\\
\hline
1 & 9.740346466 & 3.921416011\\
2 & 31.32104966 & 2.332399296\\
3 & 5.883284899 & 2.711148601\\
4 & 8.483479693 & 3.029508192\\
5 & 3.02759275 & 1.949908716\\
6 & 6.221257613 & 2.338482549\\
7 & 6.909572466 & 2.351402757\\
8 & 9.928680799 & 2.519936579\\
9 & 9.152888827 & 2.537911275\\
10 & 8.425976639 & 2.496948897\\
\hline
Average: & 9.909412981 & 2.618906287\\
\hline
\end{tabular}
\caption{Clustering algorithm results with 1296 evenly spaced centroids}\label{CentroidResults1296}
\end{table}

At first, random centroids were selected for each run of the clustering algorithm on each fold. This was also done for various numbers of centroids. The runs ran for up to 15 centroids per fold. The amount of centroids that could be used was limited by the amount of time the algorithm took to run. It took approximately 12 hours to run through 2 to 15 centroids for each fold. 4 of these total runs were done. The algorithm terminated when all the datapoints assigned to each centroid was within a euclidean distance of 0.001. This number was determined experimentally. That number was the smallest distance the datapoints could be from the centroids while the algorithm would still terminate within a reasonable amount of time. The number dis dimentionless, since it is calculated using the normalised values of each indicator or dimension. Which in turn is also dimensionless. The averaged results of these random runs can be seen in Table \ref{CentroidRandomResults}.




\section{Discussion}

Since Linear regression is used often in the literature (Section \ref{regression}), the result for linear regression will be regarded as the baseline. From Table \ref{OLSResults} we can see that the Error value averaged 9.609 years iver all the data sets.The large error is to be expected since the data is not linear, as can be seen in Figure \ref{scatter}. More specifically, the Preston Curve discussed in Section \ref{income} can be seen in Figure \ref{scatter} where Life Expectancy and GDP per Capita intersect. Thus Linear Regression when done over the whole dataset, gives a result with such high error. 

The kNN-alogortihm fared much better. The kNN-algorithm was first implemented without weighting the nearest neighbours. Table \ref{kNNResultsUnweighted} shows that result. This table only takes the Average Mean Error into account for the value of k for which the Average Mean Error is the smallest. The value of k does change per run but averages around 39. Over the 10 the best value of the Average Mean Error averaged 2.12 years. 

The weighted version of the kNN algorithm was them implemented in order to improve on the numbers found in Table \ref{kNNResultsUnweighted}. The results for the 10 runs of the Weighted kNN-algorithm is shown in Table \ref{kNNResultsWeighted}. 

The weighted version of the algorithm did improve on the results of the unweighted algorithm. The weitghted algorithm had an average Mean Error of 2.115 years. Which is an improvement of 0.0051 years. THe standard deviatio of both kNN-Algorithms also improved more than 3 fold over the standard deviation of the errors for the OLS-Regression algorithm.

Unlike OLS Linear regression, the kNN-algorithm only takes datapoints into account that are local to the point in question. Therefor, not caring about the general structure of the whole dataset. One disadvantage of this algorithms was the need to run the data for so many different values of k. The max value of k was 106. This meant that for each fold the kNN-algorithm was run for 1 Nearest Neighbour up to all the Neighbours in the fold. Given that a fold only contained 106 data points. 

As discussed in Section \ref{clustering}, the CLR algorithm assigns data points to various clusters and then updates the position of the centroid of each cluster based on the average position of all the assigned datapoints. When some form of equalibrium is reached, Linear regression is applied to each of the clusters. The same code that was used in the OLS regression algorithm above, was used for the Linear Regression part of the CLR algorithm.

The first results from the custering algorithm was where the optimum number of clusters was being determined using random positions of the centroids. Unfortunately, as discussed in Section \ref{Clustering Results}, the number of centroids were varied between 2 and 15. A plot of this data can be seen in Figure \ref{clusteringRandom}. There is no obvious pattern in the data from this graph. Unlike the graphs for the kNN algorithms where accuracy clearly increased to a point around k=40. This can be seen in Figure \ref{kNNResultsPerFold}. Also we can see from Table \ref{CentroidRandomResults} that the average error, from randomly trying to find the correct initial positions of the centroids, is also worse that the average error achieved by the weighted kNN-algorithm. This data does not prove that it is impossible to find good initial positions for a small number of centroids randomly. A greater number of runs would have had to be performed in order to have gotten a sense of what randomised initial position could have delivered in terms of better modelling the data. The number of runs were also reduced when compared to the other algorithms (see Section \ref{Clustering Results}). Which does not make this a fair comparison.


\begin{figure}
\centering
\includegraphics[height=14cm, width=14cm]{clusteringRandomPlot.png}
\caption{Average Error vs the number of randomly initiated centroids}
\label{clusteringRandom}
\end{figure}

Randomly trying to find appropriate initial starting position for the centroids was not successful. Therefor evenly spaced initial centroids were chosen. If the starting points were evenly spaced over the problem domain, then outlying data points should have their own centroids, thus improving predictave performance.

The next question was how small should those intervals be. Larger intervals means fewer centroids and will improve the running performance of the algorithm and enable more runs over more folds, but will pair data points that are more different together. Which makes the predictave performance worse. Conversely, a smaller interval leads to more centroids. Which would allow only more similar data points to be grouped together and thus improve predictave performance of the algorithm, but at the cost of running performance. 

% From a practical point of view, this new evely spaced form of CLS should not run more than 12 hours. That was the amount of time the random version of CLS ran for per fold. Thus a full run should be approximately 120 hours, which is 5 full days. The random version of CLS also had to do the calculations for $\sum_{2}^{15} i = 119$

If an interval of 0.1 is chosen in normalised space, then that would mean $11^4 = 14641$ centroids that have to be processed, because the space is 4 dimentional. Therefor, an interval of 0.2 was chosen. Which equates to 1296 centroids. A much more reasonable number. This one run took about a week and was done for all 10 folds but one one run. The results (Table \ref{CentroidResults1296}) show an average error of 9.909. Which is worse than even the Linear regression that was done first. This can be explained by some of the spaces (created by the intervals) have fewer predictors than dimensions of the space. In this case $\beta$ (see Section \ref{regression}) has no unique solutions. Thus giving unexpected results. A good example is where in fold 2 the test point for Norway in 2014 had a predicted life expectancy of 1126.056 years while in reality the life expectancy was 82.1 years. 




no of runs arent enough like 10 for knn and OLS

Trying to randomly find the optimum number of centroids and the optimum initial placement of these centroids, did not yield better results than the kNN algorithm (See Table \ref{kNNResultsWeighted}). Having more runs could have yeilded better results. But, given the limited computing power available, another approach was followed.

Table of optimum run random clustering vs average mean error

dimension explosion

delta of 0.2 

1296

0.0,0.0,0.0,0.0 to 1.0,1.0,1.0,1.0




\section{Conclusions and limitations}



Multi-dimentional data is hard to visualise and thus hard to understand and study. The situation gets worse with each additional dimention. Combining Primary- , Secondary- and Tertiary School Enrollment, meant that the amount of dimentions studied reduced from 7 to 5. Which is unfortunately still difficult to analyse, mostly becuase it is impossible to visualise mentally. Techniques like a matrix scatter plot from Figure \ref{scatter} helped but still require interpretation.

The source code and raw results can be found on GitHub \citep{rawData}.

The greatest limitation for the analysis of the centroid algorithm was the computation time. Running through all the folds of a dataset could take more than a week, not icluding external factors like load shedding or technical issues on the chosen computer. Python is not an ideal language to scientific computation when all the computation, especially long loops, are not being done in a more performant language like c or c++ \citep{Cai2005}. It would have been more efficient to implement the clustering algorithm in a more performant language and then calling those libraries from python. If that was the case then the algorithm could have been tested against more clusters. The threshold distance that the datapoints had to be from the centroids, for the algorithm to terminate, could also have been reduced. 

Multiple CPU cores should also have been taken into account. The algorithm only ran on one core of a 4-core I7-4770k, which also supports multi-threading. Multiple runs of the algorithm could have been done in parallel since they are mutually exclusive. 


\section{Recommendations}









%\section{Analysis}


%\cite{Murphy} p23 how to compare techniques
%\section{Conclusion}

%\section{Recommendations}


%\appendix
%\appendixpage

%\addappheadtotoc
\addcontentsline{toc}{section}{References}
\bibliography{mybib}



\end{document}
