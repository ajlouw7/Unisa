\documentclass[10pt,a4paper]{article}
%\usepackage[natbibapa]{apacite} 
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{physics}
\usepackage[a4paper,margin=1in]{geometry} 
\usetikzlibrary{automata,positioning}
%\bibliographystyle{apacite}

\usetikzlibrary{shapes.misc}
\usetikzlibrary{decorations.pathreplacing}

\title{Natural Language Processing (COS4861)}
\author{Adriaan Louw (53031377)}


\tikzset{basic/.style={draw,fill=blue!50!green!20,
                       text badly centered,minimum width=3em}}
\tikzset{input/.style={basic,circle}}
\tikzset{weights/.style={basic,rectangle,minimum width=2em}}
\tikzset{functions/.style={basic,circle,fill=blue!50!green!20}}
\tikzset{dummy/.style    = {circle,draw}}
\newcommand{\addsymbol}{\draw[thick] (0.5em,0.5em) -- (0,0.5em) -- 
                        (0,-0.5em) --  (-0.5em,-0.5em)
                        (0em,0.75em) -- (0em,-0.75em)
                        (0.75em,0em) -- (-0.75em,0em);}
                        
\begin{document}

\maketitle

\section{Question 1}
\subsection{Question 1.1}
The Markov assumption is that for a model the future state of the model depends only on the current state of the model and not any previous state. This assumption is used in bigrams. Where the probability that a word will appear is dependant only on the previous word i.e. $P(w_n|w_{n-1})$. This is not true for n-grams where $n >2$ since then the probability will depend on more than just the previous word. For example in the sentence "The cat sat on the mat" we will be using a bigram model if we wanted to determine the probability that the word "on" came after "sat" i.e. P(on|sat). Thus we are only looking at the previous word. Using a trigram P(on|cat sat) we will be looking at 2 previous "states" or words and thus not follow the markov assumption.

\subsection{Question 1.2}
Please run MLE.py and please ensure that python 3.7 is installed. It can be
found at https://www.python.org/downloads/

\subsection{Question 1.3}

I included the word "tale" in the input set to indicate an example of sparse data. Because we are using a data set there is the possibility that we want the MLE to find the most likely word following a word that is not in the training set.This makes it difficult to use an MLE on a different data set than the one it is trained on. To overcome this limitation we can use Laplace smoothing. Laplace smoothing adds one to the number of each occurrence of a the n-gram. (In this case we will be using a bigram.) Where normally the probability of finding the bigram is
\begin{equation}
P(w_n|w_{n-1}) = \frac{C(w_{n-1}w_n)}{C(w_{n-1})}
\end{equation} 

That is the number of occurrences of the bigram divided by the number of occurrences of $w_{n-1}$. This becomes

\begin{equation}
P^*(w_n|w_{n-1}) = \frac{C(w_{n-1}w_n)+1}{C(w_{n-1})+V}
\end{equation}

where V is the total vocabulary size. Coming back to our example of the word "tale" here the vocabulary will be increased by 1 and there will be an occurrence of each word in the vocabulary following "tale" added to the list of bigrams. This way the probability of any word following lame will not be 0.
 
\section{Question 2}
For $V_2(1):$

\begin{equation}
\begin{split}
V_1(1)P(PPSS\mid PPSS) &= (0.025)(0.00014) \\
&= 0.0000035
\end{split}
\end{equation}

\begin{equation}
\begin{split}
V_1(2)P(PPSS\mid VB) &= (0)(0.007) \\
 &= 0
\end{split}
\end{equation}

\begin{equation}
\begin{split}
V_1(3)P(PPSS\mid TO) &= (0)(0) \\
&= 0
\end{split}
\end{equation}

\begin{equation}
\begin{split}
V_1(4)P(PPSS\mid NN) &= (0)(0.0045) \\
 &= 0
\end{split}
\end{equation}

\begin{equation}
\begin{split}
V_2(1) &= max(0.0000035,0,0,0)P(want\mid PPSS) \\
 &=(0.0000035)(0) \\
 &=0
\end{split}
\end{equation}

For $V_2(3):$

\begin{equation}
\begin{split}
V_1(1)P(TO\mid PPSS) &= (0.025)(0.00079) \\
&= 0.00001975
\end{split}
\end{equation}

\begin{equation}
\begin{split}
V_1(2)P(TO\mid VB) &= (0)(0.035) \\
 &= 0
\end{split}
\end{equation}

\begin{equation}
\begin{split}
V_1(3)P(TO\mid TO) &= (0)(0) \\
&= 0
\end{split}
\end{equation}

\begin{equation}
\begin{split}
V_1(4)P(TO\mid NN) &= (0)(0.016) \\
 &= 0
\end{split}
\end{equation}

\begin{equation}
\begin{split}
V_2(3) &= max(0.00001975,0,0,0)P(want\mid TO) \\
 &=(0.00001975)(0) \\
 &=0
\end{split}
\end{equation}




For $V_2(4):$

\begin{equation}
\begin{split}
V_1(1)P(NN\mid PPSS) &= (0.025)(0.0012) \\
&= 0.00003
\end{split}
\end{equation}

\begin{equation}
\begin{split}
V_1(2)P(NN\mid VB) &= (0)(0.047) \\
 &= 0
\end{split}
\end{equation}

\begin{equation}
\begin{split}
V_1(3)P(NN\mid TO) &= (0)(0.00047) \\
&= 0
\end{split}
\end{equation}

\begin{equation}
\begin{split}
V_1(4)P(NN\mid NN) &= (0)(0.087) \\
 &= 0
\end{split}
\end{equation}

\begin{equation}
\begin{split}
V_2(4) &= max(0.00003,0,0,0)P(want\mid NN) \\
 &=(0.00003)(0.000054) \\
 &=0.00000000162
\end{split}
\end{equation}











For $V_3(1):$

\begin{equation}
\begin{split}
V_2(1)P(PPSS\mid PPSS) &= (0)(0.00014) \\
&= 0
\end{split}
\end{equation}

\begin{equation}
\begin{split}
V_2(2)P(PPSS\mid VB) &= (0.000051)(0.007) \\
 &= 0.000000357
\end{split}
\end{equation}

\begin{equation}
\begin{split}
V_2(3)P(PPSS\mid TO) &= (0)(0) \\
&= 0
\end{split}
\end{equation}

\begin{equation}
\begin{split}
V_2(4)P(PPSS\mid NN) &= (0.00000000162)(0.0045) \\
 &= 0
\end{split}
\end{equation}

\begin{equation}
\begin{split}
V_3(1) &= max(0,0.000000357,0,7.29*10^{-12})P(to\mid PPSS) \\
 &=(0.000000357)(0) \\
 &=0
\end{split}
\end{equation}

For $V_3(2):$

\begin{equation}
\begin{split}
V_2(1)P(VB\mid PPSS) &= (0)(0.23) \\
&= 0
\end{split}
\end{equation}

\begin{equation}
\begin{split}
V_2(2)P(VB\mid VB) &= (0.000051)(0.0038) \\
 &= 1.938*10^{-7}
\end{split}
\end{equation}

\begin{equation}
\begin{split}
V_2(3)P(VB\mid TO) &= (0)(0.83) \\
&= 0
\end{split}
\end{equation}

\begin{equation}
\begin{split}
V_2(4)P(VB\mid NN) &= (1.62*10^{-9})(0.0045) \\
 &= 6.48*10^{-12}
\end{split}
\end{equation}

\begin{equation}
\begin{split}
V_3(2) &= max(0,1.938*10^{-7},0,6.48*10^{-12})P(to\mid VB) \\
 &=(1.938*10^{-7})(0) \\
 &=0
\end{split}
\end{equation}

For $V_3(3):$

\begin{equation}
\begin{split}
V_2(1)P(TO\mid PPSS) &= (0)(0.00079) \\
&= 0
\end{split}
\end{equation}

\begin{equation}
\begin{split}
V_2(2)P(TO\mid VB) &= (0.000051)(0.0035) \\
 &= 0.000001785
\end{split}
\end{equation}

\begin{equation}
\begin{split}
V_2(3)P(TO\mid TO) &= (0)(0) \\
&= 0
\end{split}
\end{equation}

\begin{equation}
\begin{split}
V_2(4)P(TO\mid NN) &= (1.62*10^{-9})(0.016) \\
 &= 2.592*10^{-11}
\end{split}
\end{equation}

\begin{equation}
\begin{split}
V_3(3) &= max(0,0.000001785,0,2.592*10^{-11})P(to\mid TO) \\
 &=(0.000001785)(0.99) \\
 &=0.00000176715
\end{split}
\end{equation}

For $V_3(4):$

\begin{equation}
\begin{split}
V_2(1)P(TO\mid PPSS) &= (0)(0.0012) \\
&= 0
\end{split}
\end{equation}

\begin{equation}
\begin{split}
V_2(2)P(TO\mid VB) &= (0.000051)(0.047) \\
 &= 0.000002397
\end{split}
\end{equation}

\begin{equation}
\begin{split}
V_2(3)P(TO\mid TO) &= (0)(0.00047) \\
&= 0
\end{split}
\end{equation}

\begin{equation}
\begin{split}
V_2(4)P(TO\mid NN) &= (1.62*10^{-9})(0.087) \\
 &= 1.4094*10^{-10}
\end{split}
\end{equation}

\begin{equation}
\begin{split}
V_3(4) &= max(0,0.000002397,0,1.4094*10^{-10})P(to\mid NN) \\
 &=(0.000002397)(0) \\
 &=0
\end{split}
\end{equation}



For $V_4(1):$

\begin{equation}
\begin{split}
V_3(1)P(PPSS\mid PPSS) &= (0)(0.00014) \\
&= 0
\end{split}
\end{equation}

\begin{equation}
\begin{split}
V_3(2)P(PPSS\mid VB) &= (0)(0.007) \\
 &= 0
\end{split}
\end{equation}

\begin{equation}
\begin{split}
V_3(3)P(PPSS\mid TO) &= (0.00000176715)(0) \\
&= 0
\end{split}
\end{equation}

\begin{equation}
\begin{split}
V_3(4)P(PPSS\mid NN) &= (0.00000000162)(0.0047) \\
 &= 0
\end{split}
\end{equation}

\begin{equation}
\begin{split}
V_4(1) &= max(0,0,0,0)P(race\mid PPSS) \\
 &=(0)(0) \\
 &=0
\end{split}
\end{equation}

For $V_4(2):$

\begin{equation}
\begin{split}
V_3(1)P(VB\mid PPSS) &= (0)(0.023) \\
&= 0
\end{split}
\end{equation}

\begin{equation}
\begin{split}
V_3(2)P(VB\mid VB) &= (0)(0.0038) \\
 &= 0
\end{split}
\end{equation}

\begin{equation}
\begin{split}
V_3(3)P(VB\mid TO) &= (0.00000176715)(0.83) \\
&= 0.0000014667345
\end{split}
\end{equation}

\begin{equation}
\begin{split}
V_3(4)P(VB\mid NN) &= (0)(0.0040) \\
 &= 0
\end{split}
\end{equation}

\begin{equation}
\begin{split}
V_4(2) &= max(0,0,0.0000014667345,0)P(race\mid VB) \\
 &=(0.0000014667345)(0.00012) \\
 &=1.7600814*10^{-10}
\end{split}
\end{equation}


For $V_4(3):$

\begin{equation}
\begin{split}
V_3(1)P(TO\mid PPSS) &= (0)(0.00079) \\
&= 0
\end{split}
\end{equation}

\begin{equation}
\begin{split}
V_3(2)P(TO\mid VB) &= (0)(0.035) \\
 &= 0
\end{split}
\end{equation}

\begin{equation}
\begin{split}
V_3(3)P(TO\mid TO) &= (0.00000176715)(0) \\
&= 0
\end{split}
\end{equation}

\begin{equation}
\begin{split}
V_3(4)P(TO\mid NN) &= (0)(0.016) \\
 &= 0
\end{split}
\end{equation}

\begin{equation}
\begin{split}
V_4(3) &= max(0,0,0,0)P(race\mid TO) \\
 &=(0)(0) \\
 &=0
\end{split}
\end{equation}

For $V_4(4):$

\begin{equation}
\begin{split}
V_3(1)P(NN\mid PPSS) &= (0)(0.0012) \\
&= 0
\end{split}
\end{equation}

\begin{equation}
\begin{split}
V_3(2)P(NN\mid VB) &= (0)(0.047) \\
 &= 0
\end{split}
\end{equation}

\begin{equation}
\begin{split}
V_3(3)P(NN\mid TO) &= (0.00000176715)(0.00047) \\
&= 8.305605*10^{-10}
\end{split}
\end{equation}

\begin{equation}
\begin{split}
V_3(4)P(NN\mid NN) &= (0)(0.087) \\
 &= 0
\end{split}
\end{equation}

\begin{equation}
\begin{split}
V_4(4) &= max(0,0,8.305605*10^{-10},0)P(race\mid NN) \\
 &=(8.305605*10^{-10})(0.00057) \\
 &=4.73419*10^{-13}
\end{split}
\end{equation}

The path can be seen in Figure \ref{fig2}. In each step going to the node with the highest probability. Thus the pat is PPSS VB TO VB














\begin{figure}
\begin{tikzpicture}[scale=3]

    \node[input] (nn1) at (0,4) {NN};
    \node[above=1cm] at (nn1) {$V_1(4) = 0$};
    \node[input] (nn2) at (1,4) {NN};
    \node[above=1cm] at (nn2) {$V_2(4) = 1.62*10^{-9}$};
    \node[input] (nn3) at (2,4) {NN};
    \node[above=1cm] at (nn3) {$V_3(4) = 0$};
    \node[input] (nn4) at (3,4) {NN};
    \node[above=1cm] at (nn4) {$V_4(4) = 4.734*10^{-13}$};
    
    \node[input] (to1) at (0,3) {TO};
    \node[above=1cm] at (to1) {$V_1(3) = 0$};
    \node[input] (to2) at (1,3) {TO};
    \node[above=1cm] at (to2) {$V_2(3) = 0$};
    \node[input] (to3) at (2,3) {TO};
    \node[above=1cm] at (to3) {$V_3(3) = 0.00000176715$};
    \node[input] (to4) at (3,3) {TO};
    \node[above=1cm] at (to4) {$V_4(3) = 0$};
    
    \node[input] (vb1) at (0,2) {VB};
    \node[above=1cm] at (vb1) {$V_1(2) = 0$};
    \node[input] (vb2) at (1,2) {VB};
    \node[above=1cm] at (vb2) {$V_2(2) = 0.000051$};
    \node[input] (vb3) at (2,2) {VB};
    \node[above=1cm] at (vb3) {$V_3(2) = 0$};
    \node[input] (vb4) at (3,2) {VB};
    \node[above=1cm] at (vb4) {$V_4(2) = 1.7600814*10^{-10}$};

    \node[input] (ppss1) at (0,1) {PPSS};
    \node[above=1cm] at (ppss1) {$V_1(1) = 0.025$};
    \node[input] (ppss2) at (1,1) {PPSS};
    \node[above=1cm] at (ppss2) {$V_2(1) = 0$};
    \node[input] (ppss3) at (2,1) {PPSS};
    \node[above=1cm] at (ppss3) {$V_3(1) = 0$};
    \node[input] (ppss4) at (3,1) {PPSS};
    \node[above=1cm] at (ppss4) {$V_4(1) = 0$};

    \node[input] (x1) at (0,0){I};
    \node[input] (x1) at (1,0){want};
    \node[input] (x1) at (2,0){to};
    \node[input] (x1) at (3,0){race};    
    
    
    \draw[->] (ppss1) -- (vb2);
    \draw[->] (vb2) -- (to3);
    \draw[->] (to3) -- (vb4);
   % \node[input] (x2) at (-3,4){$x_2$}; 

	% inputs of h1
    %\foreach \h [count=\hi ] in {$x_2$,$x_1$,$1$}{
   % 	\node[input] (f1\hi) at (0,\hi*2cm +4cm) {\h};
   % }
    % inputs of h2
   % \foreach \h [count=\hi ] in {$x_2$,$x_1$,$1$}{
   % 	\node[input] (f2\hi) at (0,\hi*2cm -2cm) {\h};
   % }
   
\end{tikzpicture}
\caption{Showing path for question 2}
\label{fig2}
\end{figure}

\section{Question 3}
\begin{enumerate}[leftmargin=\labelsep]
\item[3.1]
\end{enumerate}

\section{Question 4}
\subsection{Question 4.1}
The parse tree for this question can be found in Figure \ref{fig41}. 

The following will be added to the lexicon. The verbs: "would", "like" and "ride". The infinitive marker "to". The preposition "with". The proper noun "Golden Arrow".

The following grammar rules will also be added:

VP $\rightarrow$ VP VP

VP $\rightarrow$ Infinitive marker VP

\tikzset{
  level 0/.style = {sibling distance=0cm},
  level 1/.style = {sibling distance=3cm},
  level 2/.style = {sibling distance=3cm},
  level 3/.style = {sibling distance=3cm}
}

\begin{figure}
\begin{tikzpicture}
 [rotate=0,
 scale = 1.4]
 
  \node [] {S}
    child { node [] {NP}  
      child { node [] {Pronoun} 
        child { node [] {I} }  } }
    child { node [] {VP} 
      child { node [] {VP}
      	child { node [] {Verb}
      	  child { node [] {would} } } }
      child { node [] {VP} 
        child { node [] {VP}
          child{ node [] {Verb}
            child{ node [] {like}}}}
        child { node [] {VP} 
          child { node [] {Infinitive marker}
            child { node [] {to}}}
          child{ node [] {VP}
            child{ node [] {Verb}
              child{ node[] {ride}}}
            child{ node [] {PP}
              child{ node [] {Preposition}
                child{ node[] {with}}}
              child{ node [] {NP}
                child{ node [] {Proper noun}
                  child{ node []{Golden Arrow}}}}}}}       
        }};
\end{tikzpicture}
\caption{Parse tree for question 4.1}
\label{fig41}
\end{figure}

\subsection{Question 4.2}

The parse tree for this question can be found in Figure \ref{fig42}.

We add the following words to the lexicon. The pronoun "What", noun "fare", and Propen nouns "Cape Town" and "Bloemfontein". 


The following grammar rules will also be added:

S $\rightarrow$ NP VP ?

VP $\rightarrow$ Verb NP PP PP

\tikzset{
  level 0/.style = {sibling distance=0cm},
  level 1/.style = {sibling distance=8cm},
  level 2/.style = {sibling distance=4cm},
  level 3/.style = {sibling distance=2cm}
}

\begin{figure}
\begin{tikzpicture}
 [rotate=0,
 scale = 1]
 
  \node [] {S}
    child{ node [] {NP}
      child{ node [] {Pronoun}
        child{ node [] {What}}}}
    child{ node [] {VP}
      child{ node [] {Verb}
        child{ node [] {is}}}
      child{ node [] {NP}
        child{ node [] {Det}
          child{ node [] {the}}}
        child{ node [] {Nominal}
          child { node [] {Noun}
            child{ node [] {fare}}}}}
      child{ node [] {PP}
        child{ node [] {Prep}
          child{ node [] {from}}}
        child{ node [] {NP}
          child{ node [] {Proper noun}
            child{ node [] {Cape Town}}}}}
      child{ node [] {PP} 
        child{ node [] {Prep}
          child{ node [] {to}}}
        child{ node [] {NP}
          child{ node [] {Propern Noun}
            child{ node [] {Bloemfontein}}}}}}
    child{ node [] {?}};

\end{tikzpicture}
\caption{Parse tree for question 4.2}
\label{fig42}
\end{figure}


\subsection{Question 4.3}

The parse tree for this question can be found in Figure \ref{fig42}.

We add the following words to the lexicon. The adverb "there", Nouns "Greyhound" and "route" and proper nouns "Durban" and "Bela-Bela". 


The following grammar rules will also be added:

S $\rightarrow$ VP NP PP PP ?

VP $\rightarrow$ VP Adverb

\tikzset{
  level 0/.style = {sibling distance=0cm},
  level 1/.style = {sibling distance=6cm},
  level 2/.style = {sibling distance=3cm},
  level 3/.style = {sibling distance=1cm}
}

\begin{figure}
\begin{tikzpicture}
 [rotate=90,
 scale = 1]
 
  \node [rotate=90] {S}
  child{ node [rotate=90] {VP}
    child{ node [rotate=90] {VP} 
      child{ node [rotate=90] {verb} 
        child { node [rotate=90] {Is}}}}
    child{ node [rotate=90] {Adverb} 
      child{ node [rotate=90] {there}}}}
  child{ node [rotate=90] {NP} 
    child{ node [rotate=90] {Det} 
      child {node [rotate=90] {a} }}
    child{ node [rotate=90] {Nominal}
      child{ node [rotate=90] {Nominal}
        child{ node [rotate=90] {Noun}
          child{ node [rotate=90] {Greyhound} } } }
      child{ node [rotate=90] {Noun} 
        child{ node [rotate=90] {route} }} }}
  child{ node [rotate=90] {PP} 
    child{ node [rotate=90] {Prep} 
      child{ node [rotate=90] {from} }}
    child{ node [rotate=90] {NP} 
      child{ node [rotate=90] {Proper noun}
        child{ node [rotate=90] {Durban}}}}}
  child{ node [rotate=90] {PP}
    child{ node [rotate=90] {Prep} 
      child{ node [rotate=90] {to}}}
    child{ node [rotate=90] {NP} 
      child{ node [rotate=90] {Proper noun}
        child{ node [rotate=90] {Bela-Bela}}}}}
  child{ node [rotate=90] {?} };

\end{tikzpicture}
\caption{Tree for question 4.3}
\label{fig43}
\end{figure}

\section{Question 5}
\subsection{Question 5.1}

$[$Rapunzel$]_{NP}[$let$]_{VP}$down$[$her goldern hair$]_{NP}$


\subsection{Question 5.2}
How$[$do$]_{VP}[$I$]_{NP}[$get$]_{VP}$to$[$Mozambique$]_{NP}$?

\subsection{Question 5.3}
$[$Can$]_{VP}[$the manager$]_{NP}[$give$]_{VP}$me$[$another room$]_{NP}$?

\end{document}