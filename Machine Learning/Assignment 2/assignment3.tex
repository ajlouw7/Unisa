\documentclass[10pt,a4paper]{article}
\usepackage[natbibapa]{apacite} 
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{physics}
\usepackage[a4paper,margin=1.2in]{geometry} 
\bibliographystyle{apacite}


\title{Assignment 2 Machine Learning COS4852}
\author{ Adriaan Louw (53031377) }

\tikzset{
  treenode/.style = {shape=rectangle, rounded corners,
                     draw, align=center,
                     top color=white, bottom color=blue!20},
  root/.style     = {treenode, font=\Large, bottom color=red!30},
  env/.style      = {treenode, font=\ttfamily\normalsize},
  dummy/.style    = {circle,draw}
}

\newcommand{\eeq}[1]{\begin{equation}
\begin{split}
#1
\end{split}
\end{equation}}

\usetikzlibrary{shapes.misc}
\usetikzlibrary{decorations.pathreplacing}

\tikzset{cross/.style={cross out, draw=black, minimum size=20*(#1-\pgflinewidth), inner sep=0pt, outer sep=0pt}, 
%default radius will be 1pt. 
cross/.default={1pt}}


\tikzset{basic/.style={draw,fill=blue!50!green!20,
                       text badly centered,minimum width=3em}}
\tikzset{input/.style={basic,circle}}
\tikzset{weights/.style={basic,rectangle,minimum width=2em}}
\tikzset{functions/.style={basic,circle,fill=blue!50!green!20}}
\newcommand{\addsymbol}{\draw[thick] (0.5em,0.5em) -- (0,0.5em) -- 
                        (0,-0.5em) --  (-0.5em,-0.5em)
                        (0em,0.75em) -- (0em,-0.75em)
                        (0.75em,0em) -- (-0.75em,0em);}
\begin{document}

\maketitle

\tableofcontents

\section{Question 1}
\subsection{Question 1(a)}

Firstly we calculate the line 

\begin{equation}
\label{line}
 x_2 = mx_1 + c
\end{equation}

for the intersect points (2,0) and (0,6).

Calculating slope m,

\begin{equation}
\begin{split}
m &= \frac{6-0}{0-2} \\
  &= -3\\
\end{split}
\end{equation}

$x_2$ intercept $c$ is 6.

This makes equation \ref{line}

\begin{equation}
\label{pop}
x_2 = -3x_1 + 6
\end{equation}

\cite{nils} gives the equation for the hyperplane as

\begin{equation}
\sum_{i=1}^n x_i\omega_i \geq \theta
\end{equation}

which in this case gives the equation for the hyperplane to be

\begin{equation}
\label{weightline}
\omega_1x_1 + \omega_2x_2 +\omega_3 = 0
\end{equation}

We need to get equation \ref{weightline} in the form of equation \ref{line}

\begin{equation}
\begin{split}
\label{simp}
\omega_1x_1 + \omega_2x_2 + +\omega_3 &= 0\\
\omega_2x_2 &= -\omega_1x_1 - \omega_3\\ 
x_2 &= -\frac{\omega_1x_1}{\omega_2} - \frac{\omega_3}{\omega_2} \\ 
\end{split}
\end{equation}

Comparing coefficients m and c from equation \ref{pop} to \ref{simp} we get

\begin{equation}
\begin{split}
-\frac{\omega_1}{\omega_2} &= -3 \\
\omega_1 &= 3\omega_2\\ 
\end{split}
\end{equation}

and

\begin{equation}
\begin{split}
-\frac{\omega_3}{\omega_2} &= 6\\
\omega_3 &= -6\omega_2\\ 
\end{split}
\end{equation}

If we choose $\omega_3 = -2$ then $\omega_1=1$ and $\omega_2 = \frac{1}{3}$. This makes the hyperplane equation from equation \ref{weightline}

\begin{equation}
x_1 + \frac{x_2}{3} -2 = 0
\end{equation}

Now we need to test this hyperplane. For positive instance (2,6)

\begin{equation}
\begin{split}
x_1 + \frac{x_2}{3} - 2 &= \\
2 + \frac{6}{2} - 2 &= \\
2&\\
\end{split}
\end{equation}

Which is as expected.

And the negative instance (-1,2)

\begin{equation}
\begin{split}
x_1 + \frac{x_2}{3} - 2 &= \\
-1 + \frac{2}{3} -2 & = \\
-\frac{7}{3}
\end{split}
\end{equation}

This is also as expected. The perceptron now classifies the the data correctly
\subsection{Question 1(b)}

\begin{center}
\begin{tikzpicture}[scale=0.6]
%\draw [help lines] (-3,-3) grid (3,3);
% Euclidean
\draw [<->](0,-8)--(0,8) node[right]{$X_2$};
\draw [<->](-8,0)--(8,0) node[right]{$X_1$};
 
%labels
\foreach \x in {-8,-6,-4,-2,2,4,6,8}
     \draw (\x,1pt) -- (\x,-3pt) node[anchor=north] {$\x$};

\foreach \y/\ytext in {-8,-6,-4,-2,2,4,6,8}
     \draw (1pt,\y) -- (-3pt,\y) node[anchor=east] {$\y$};
 
\draw (-1,2) node[cross,green]{};
\draw (1,-2) node[cross,green]{};
\draw (-2,-5) node[cross,green]{};
\draw (-5,-2) node[cross,green]{}; 

\draw (-5,6) node[cross,red]{};
\draw (3,5) node[cross,red]{};
\draw (2,-2) node[cross,red]{};
\draw (5,-5) node[cross,red]{};

\draw [blue] (-8,8) -- (8,-8);
\draw [teal] (-8,66/7) -- (8,-62/7);
 
\end{tikzpicture}
\end{center}

From the above image we can see that any that it is not possible to create a hyperplane that correctly classifies all negative instances and positive instances. The blue line is the line $x_2 = -x_1$ and the teal line is the line $x_2 = -\frac{7}{8}x_1 + \frac{2}{7}$. The any minimum plane that correctly classifies all the negative instances will classify the positive instance (-1,2) incorrectly as negative. 

We can create a hyperplane from regression from all the points close to where the hyperplane should be. Using negative points (-5,6),(2,-2),(5,-5) and positive points (-1,2),(1,-2)

For the equation of the regressed line $x_2=mx_1+c$

\begin{equation}
\begin{split}
\label{m}
m &= r\frac{S(x_2)}{S(x_1)}\\ 
m &= \frac{\sum((x_1 - \bar{x_1})(x_2-\bar{x_2}))}{\sqrt{\sum (x_1-\bar{x_1})^2 \sum(x_2-\bar{x_2})^2}} \frac{\sqrt{\frac{\sum (x_2-\bar{x_2})^2}{n-1}}}{\sqrt{\frac{\sum (x_1-\bar{x_1})^2}{n-1}}}\\
\end{split}
\end{equation}

where r is Pearsons Correlation Coefficient and S is standard deviation of axis $x_2$ or $x_1$.

Here follows the calculation

\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
$x_1$ & $x_2$ & $x_1-\bar{x_1}$ & $x_2-\bar{x_2}$ &$(x_1-\bar{x_1})(x_2-\bar{x_2})$ & $(x_1-\bar{x_1})^2$ & $(x_2-\bar{x_2})^2$ \\ 
\hline
-5 &  6 & -5.4 &  6.2 & -33.48 & 29.16 & 38.44\\
 2 & -2 &  1.6 & -1.8 &  -2.88 &  2.56 &  3.24\\
 5 & -5 &  4.6 & -4.8 & -22.08 & 21.16 & 23.04\\
-1 &  2 & -1.4 &  2.2 &  -3.08 &  1.96 &  4.84\\
 1 & -2 &  0.6 & -1.8 &  -1.08 &  0.36 &  3.24\\
\hline
\end{tabular}

From the above table we have $\bar{x_1} = 0.4, \bar{x_2} = -0.2, \sum((x_1 - \bar{x_1})(x_2-\bar{x_2})) = -62.6, (x_1-\bar{x_1})^2 = 55.2,and (x_2-\bar{x_2})^2 = 72.8$ 

Passing these into equation \ref{m} we get m = -1.13

\begin{equation}
\begin{split}
c &= \bar{x_2} - m\bar{x_1}\\
  &= -0.2 - (-1.13)(0.4)\\
  &= 0.25\\  
\end{split}
\end{equation}

Which gives us equation 

\begin{equation}
x_2 = -1.13x_1 + 0.25
\end{equation}

Visually it would be

\begin{center}
\begin{tikzpicture}[scale=0.6]
%\draw [help lines] (-3,-3) grid (3,3);
% Euclidean
\draw [<->](0,-8)--(0,8) node[right]{$X_2$};
\draw [<->](-8,0)--(8,0) node[right]{$X_1$};
 
%labels
\foreach \x in {-8,-6,-4,-2,2,4,6,8}
     \draw (\x,1pt) -- (\x,-3pt) node[anchor=north] {$\x$};

\foreach \y/\ytext in {-8,-6,-4,-2,2,4,6,8}
     \draw (1pt,\y) -- (-3pt,\y) node[anchor=east] {$\y$};
 
\draw (-1,2) node[cross,green]{};
\draw (1,-2) node[cross,green]{};
\draw (-2,-5) node[cross,green]{};
\draw (-5,-2) node[cross,green]{}; 

\draw (-5,6) node[cross,red]{};
\draw (3,5) node[cross,red]{};
\draw (2,-2) node[cross,red]{};
\draw (5,-5) node[cross,red]{};

\draw [blue] (-8,9.32) -- (8,-8.81);
 
\end{tikzpicture}
\end{center}

This will minimise the error even though it would incorrectly classify (-1,2)

\subsection{Question 1(c)}

The equation for the hyperplane is: 

\begin{equation}
\label{3d}
\omega_1 x_1 + \omega_2 x_2 + \omega_3 x_3 + \omega_0 = 0
\end{equation}

Passing in the point (14.0,0) gives us into equation \ref{3d}
\begin{equation}
\begin{split}
\omega_1 x_1 + \omega_2 x_2 + \omega_3 x_3 + \omega_0 &= 0 \\
\omega_1(14) + \omega_2(0) + \omega_3(0) + \omega_0 &=0\\
14\omega_1 + \omega_0 &= 0\\
\omega_1 &= -\frac{\omega_0}{14}\\
\end{split}
\end{equation}

Passing the point (0,8,0) into equation \ref{3d}
\begin{equation}
\begin{split}
\omega_1 x_1 + \omega_2 x_2 + \omega_3 x_3 + \omega_0 &= 0 \\
\omega_1(0) + \omega_2(8) + \omega_3(0) + \omega_0 &=0\\
8\omega_2 + \omega_0 &= 0\\
\omega_2 &= -\frac{\omega_0}{8}\\
\end{split}
\end{equation}

Passing the point (0,0,6) into equation \ref{3d}
\begin{equation}
\begin{split}
\omega_1 x_1 + \omega_2 x_2 + \omega_3 x_3 + \omega_0 &= 0 \\
\omega_1(0) + \omega_2(0) + \omega_3(6) + \omega_0 &=0\\
6\omega_3 + \omega_0 &= 0\\
\omega_3 &= -\frac{\omega_0}{6}\\
\end{split}
\end{equation}

Now we need to solve the 3 simultaneous equations. Any value of $\omega_0$ will solve the equation but we still need to check for polarity

If $\omega_0 = -1$

Then $\omega_1 = \frac{1}{14}$,$\omega_2=\frac{1}{8}$ and $\omega_3 = \frac{1}{6}$.

Checking for polarity with positive instance (3,5,5)

\begin{equation}
\begin{split}
\omega_1 x_1 + \omega_2 x_2 + \omega_3 x_3 + \omega_0 &= \\
\frac{1}{14}(3) + \frac{1}{8}(5) + \frac{1}{6}(5) -1 &= \\
\frac{36 + 105 + 140 -168}{168} &= \\
\frac{113}{168}\\
\end{split}
\end{equation}

Which is positive thus meaning the polarity is correct.

Thus $\omega_0 = -1; \omega_1 = \frac{1}{14} ; \omega_2=\frac{1}{8}; \omega_3 = \frac{1}{6}$.

\subsection{Question 1(d)}

  \begin{tikzpicture}[scale=1.2]
    \foreach \h [count=\hi ] in {$x_3$,$x_2$,$x_1$,$1$}{%
          \node[input] (f\hi) at (0,\hi*2cm-5 cm) {\h};
        }
    \node[functions] (sum) at (4,0) {$\sum$};
    \foreach \h [count=\hi ] in {$w_3=\frac{1}{6}$,$w_2=\frac{1}{8}$,$w_1=\frac{1}{14}$,$w_0=-1$}{%
          \path (f\hi) -- node[weights] (w\hi) {\h} (sum);
          \draw[->] (f\hi) -- (w\hi);
          \draw[->] (w\hi) -- (sum);
        }        
    \node[functions] (step) at (7,0) {};
       \begin{scope}[xshift=7cm,scale=.75]
         \addsymbol
       \end{scope}
    \draw[->] (sum) -- (step);
    \draw[->] (step) -- ++(1,0);
    % Labels
    \node[above=1cm] at (f4) {inputs};
    \node[above=1cm] at (w4) {weights};
    \node at (5.5,0.4) {net};
    \node[below=2cm] at (sum){$net = \sum_{i=0}^3 \omega_1 x_i$};
    \node at (9,0) {output};
    \draw [decorate,decoration={brace,amplitude=10pt},xshift=-4pt,yshift=0pt]
          (7.0,1.5) -- (7.0,3) node [black,midway,xshift=-0.6cm] 
          {\footnotesize $o$};
    \node at (7.9,2.5) {1 if $net > 0$};
    \node at (7.9,1.9) {-1 if $net < 0$};
    \end{tikzpicture}


\section{Question 2}
\subsection{Question 2(a)}

The truth table for this function is
 
\begin{tabular}{|c|c|c|c|}
\hline
$x_1$ & $x_2$ & $\neg x_1$ & $\neg x_1 \vee x_2$ \\
\hline
-1 &-1 &1 &-1\\
-1 &1 &1 &1\\
1 &-1 &-1 &-1\\
1 &1 &-1 &-1\\
\hline
\end{tabular}

Visually this is:

\begin{center}
\begin{tikzpicture}[scale=3.0]
%\draw [help lines] (-3,-3) grid (3,3);
% Euclidean
\draw [<->](0,-1.1)--(0,1.1) node[right]{$x_2$};
\draw [<->](-1.1,0)--(1.1,0) node[right]{$x_1$};
 
\draw[teal,thick,dashed] (-1.0,-0.5) -- (0.5,1.0);

%labels
\foreach \x in {-1,1}
     \draw (\x,1pt) -- (\x,-3pt) node[anchor=north] {$\x$};

\foreach \y/\ytext in {-1,1}
     \draw (1pt,\y) -- (-3pt,\y) node[anchor=east] {$\y$};
 
\draw (-1,-1) node[cross,red]{};
\draw (-1,1) node[cross,green]{};
\draw (1,-1) node[cross,red]{};
\draw (1,1) node[cross,red]{}; 

\end{tikzpicture}
\end{center}

The dashed line represents the function

\begin{equation}
x_2 = x_1 + 0.5
\end{equation}

This hyperplane will classify the boolean function correctly because it linearly separates all the positive instances from the negative ones.

The equation for the hyperplane with weights are given by equation \ref{weightline} and we know from equation \ref{simp} what the relationship is from the weight equation to the line equation. Thus for case

\begin{equation}
\begin{split}
m &= -\frac{\omega_1}{\omega_2} \\
1 &= -\frac{\omega_1}{\omega_2} \\
\omega_2 &= -\omega_1 \\
\end{split}
\end{equation}

and

\begin{equation}
\begin{split}
c &= -\frac{\omega_0}{\omega_2}\\
0.5 &= -\frac{\omega_0}{\omega_2}\\
\omega_0 &= -\frac{\omega_2}{2}\\
\end{split}
\end{equation}

Now we have the relationship between the weights. If we try $\omega_0 = 1$ we get $\omega_1 = 2$ and $\omega_2 = -2$ 

Which makes our weight equation

\begin{equation}
2x_1 - 2x_2 + 1
\end{equation}

Testing for polarity with point (-1,-1) we get
\begin{equation}
\begin{split}
2(-1) -2(-1) + 1 &= 1
\end{split}
\end{equation}

which indicates a positive result but we expect a negative result. Thus our polarity is wrong

We try $\omega_0 = -1$ which gives $\omega_1 = -2$ and $\omega_2 = 2$

Which makes the weight equation

\begin{equation}
-2x_1 + 2x_2 - 1
\end{equation}

Trying all our data we can sum it up in the following table

\begin{tabular}{|c|c|c|}
\hline
$x_1$ & $x_2$ & Result \\
\hline
-1 &-1 & -1\\
-1 & 1 & 4\\
1  &-1 & -5\\
1  & 1 & -1 \\
\hline
\end{tabular}

This shows that weights $\omega_0 = -1$; $\omega_1 = -2$ and $\omega_2 = 2$ correctly classify the data

 \begin{tikzpicture}[scale=1]
    \foreach \h [count=\hi ] in {$x_2$,$x_1$,$1$}{%
          \node[input] (f\hi) at (0,\hi*2cm-4 cm) {\h};
        }
    \node[functions] (sum) at (5,0) {$\sum$};
    \foreach \h [count=\hi ] in {$w_2 =2$,$w_1 = -2$,$w_0 = -1$}{%
          \path (f\hi) -- node[weights] (w\hi) {\h} (sum);
          \draw[->] (f\hi) -- (w\hi);
          \draw[->] (w\hi) -- (sum);
        }        

    \node[functions] (step) at (7,0) {};
       \begin{scope}[xshift=7cm,scale=.75]
         \addsymbol
       \end{scope}
    \draw[->] (sum) -- (step);
    \draw[->] (step) -- ++(1,0);
    % Labels
    \node[above=1cm]  at (f3) {inputs};
    \node[above=1cm] at (w3) {weights};
    \node[right=2cm] at (step) {output};
        \node at (6,0.4) {net};
    \node[below=2cm] at (sum){$net = \sum_{i=0}^2 \omega_1 x_i$};
    \draw [decorate,decoration={brace,amplitude=10pt},xshift=-4pt,yshift=0pt]
          (7.0,1.5) -- (7.0,3) node [black,midway,xshift=-0.6cm] 
          {\footnotesize $o$};
    \node at (7.9,2.5) {1 if $net > 0$};
    \node at (7.9,1.9) {-1 if $net < 0$};
    \end{tikzpicture}


















\subsection{Question 2(b)}

\begin{tabular}{|c|c|c|c|}
\hline
$x_1$ & $x_2$ & $x_1 \bigoplus x_2$ & $f_2 =\neg (x_1 \bigoplus x_2)$ \\
\hline
-1 &-1 &-1 &1\\
-1 &1 &1 &-1\\
1 &-1 &1 &-1\\
1 &1 &-1 &1\\
\hline
\end{tabular}

Visually this is:

\begin{center}
\begin{tikzpicture}[scale=3.0]
%\draw [help lines] (-3,-3) grid (3,3);
% Euclidean
\draw [<->](0,-1.1)--(0,1.1) node[right]{$x_2$};
\draw [<->](-1.1,0)--(1.1,0) node[right]{$x_1$};
 
\draw[red,thick,dashed] (-1.0,-0.5) -- (0.5,1.0);
\draw[red,thick,dashed] (-0.5,-1.0) -- (1.0,0.5);

%labels
\foreach \x in {-1,1}
     \draw (\x,1pt) -- (\x,-3pt) node[anchor=north] {$\x$};

\foreach \y/\ytext in {-1,1}
     \draw (1pt,\y) -- (-3pt,\y) node[anchor=east] {$\y$};
 
\draw (-1,-1) node[cross,green]{};
\draw (-1,1) node[cross,red]{};
\draw (1,-1) node[cross,red]{};
\draw (1,1) node[cross,green]{}; 

\end{tikzpicture}
\end{center}

We can see from the previous diagram that $f_2$ cannot be linearly separated by 1 hyperplane. In other words a single perceptron cannot classify this function. We need to try to decompose $f_2$ into multiple linearly separable functions that can be modelled by multiple perceptrons.

\begin{equation}
\begin{split}
f_2 &= \neg ( x_1 \bigoplus x_2 ) \\
    &= \neg( (x_1 \wedge \neg x_2) \vee (\neg x_1 \wedge x_2))\\
    &= \neg(x_1\wedge \neg x_2) \wedge \neg(\neg x_1 \wedge x_2)\\
    &= (\neg x_1 \vee x_2) \wedge (x_1 \vee \neg x_2)\\
\end{split}
\end{equation}  

We can thus create new function which is equivalent to $f_2$

\begin{equation}
g(h_1,h_2) = h_1 \wedge h_2 
\end{equation} 

where 

\begin{equation}
h_1 = \neg x_1 \vee x_2
\end{equation}

and 

\begin{equation}
h_2 = x_1 \vee \neg x_2
\end{equation}

The truth table will thus become

\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
$x_1$ & $x_2$ & $x_1 \bigoplus x_2$ & $f_2 =\neg (x_1 \bigoplus x_2)$ & $h_1$ & $h_2$ &$g(h_1,h_2)$\\
\hline
-1 &-1 &-1 & 1 & 1 & 1 & 1  \\
-1 & 1 & 1 &-1 & 1 &-1 & -1 \\
 1 &-1 & 1 &-1 &-1 & 1 & -1 \\
 1 & 1 &-1 & 1 & 1 & 1 & 1  \\
\hline
\end{tabular}

Drawing $h_1$ gives

\begin{center}
\begin{tikzpicture}[scale=3.0]
%\draw [help lines] (-3,-3) grid (3,3);
% Euclidean
\draw [<->](0,-1.1)--(0,1.1) node[right]{$x_2$};
\draw [<->](-1.1,0)--(1.1,0) node[right]{$x_1$};
 
%\draw[red,thick,dashed] (-1.0,-0.5) -- (0.5,1.0);
\draw[red,thick,dashed] (-0.5,-1.0) -- (1.0,0.5);

%labels
\foreach \x in {-1,1}
     \draw (\x,1pt) -- (\x,-3pt) node[anchor=north] {$\x$};

\foreach \y/\ytext in {-1,1}
     \draw (1pt,\y) -- (-3pt,\y) node[anchor=east] {$\y$};
 
\draw (-1,-1) node[cross,green]{};
\draw (-1,1) node[cross,green]{};
\draw (1,-1) node[cross,red]{};
\draw (1,1) node[cross,green]{}; 

\end{tikzpicture}
\end{center}

Using equation \ref{weightline} and knowing this hyperplane goes through points (-0.5,-1) and (1.0,0.5).
We have  
\begin{equation}
\label{2bh1p1}
\begin{split}
\omega_1(-0.5) + \omega_2(-1) + \omega_0 &= 0 \\
\frac{-\omega_1}{2} - \omega_2 + \omega_0 &= 0 \\
\end{split}
\end{equation}
for the point (-0.5,-1) and

\begin{equation}
\label{2bh1p2}
\begin{split}
\omega_1(1) + \omega_2(0.5) + \omega_0 &= 0 \\
\omega_1 + \frac{\omega_2}{2} + \omega_0 &= 0 \\
\end{split}
\end{equation}

for point (1.0,0.5)

Equating equations \ref{2bh1p1} and \ref{2bh1p2} we get

\begin{equation}
\label{2bh1i}
\begin{split}
\frac{-\omega_1}{2} - \omega_2 + \omega_0 &= \omega_1 + \frac{\omega_2}{2} + \omega_0\\
\frac{3\omega_1}{2} &= \frac{3\omega_2}{2}\\
\omega_2 =& - \omega_1\\
\end{split}
\end{equation}

Passing equation \ref{2bh1i} into \ref{2bh1p2} we get

\begin{equation}
\begin{split}
\omega_1 + \frac{-\omega_1}{2} + \omega_0 &= 0 \\
\omega_0 &= -\frac{\omega_1}{2}\\
\end{split}
\end{equation}

If we take $\omega_0 = 1$ then $\omega_1 = -2$ and $\omega_2 = 2$.

Testing for polarity we use the point (-1,-1) where we expect a positive answer 

For point (-1,-1)

\begin{equation}
\begin{split}
-2x_1+2x_2 + 1 &=\\
-1(-1) + 2(-1) + 1& = 1\\
\end{split}
\end{equation}

Which is greater than 0 as expected. Thus the weights for $h_1$ is $\omega_0 = 1; \omega_1 = -2; \omega_2 = 2$ 

Now drawing $h_2$

\begin{center}
\begin{tikzpicture}[scale=3.0]
%\draw [help lines] (-3,-3) grid (3,3);
% Euclidean
\draw [<->](0,-1.1)--(0,1.1) node[right]{$x_2$};
\draw [<->](-1.1,0)--(1.1,0) node[right]{$x_1$};
 
\draw[red,thick,dashed] (-1.0,-0.5) -- (0.5,1.0);
%\draw[red,thick,dashed] (-0.5,-1.0) -- (1.0,0.5);

%labels
\foreach \x in {-1,1}
     \draw (\x,1pt) -- (\x,-3pt) node[anchor=north] {$\x$};

\foreach \y/\ytext in {-1,1}
     \draw (1pt,\y) -- (-3pt,\y) node[anchor=east] {$\y$};
 
\draw (-1,-1) node[cross,green]{};
\draw (-1,1) node[cross,red]{};
\draw (1,-1) node[cross,green]{};
\draw (1,1) node[cross,green]{}; 

\end{tikzpicture}
\end{center}

This is a similar perceptron from as in Question 2a. It uses the same hyperplane but the polarity is reversed. Thus from question 2a $\omega_2 = -\omega_1; \omega_0 = -\frac{\omega_2}{2}$. 

Testing polarity with $\omega_0 = 1$ then $\omega_1=2;\omega_2=-2$. This gives with positive instance (-1,-1)

\begin{equation}
\begin{split}
2x_1 - 2x_2 +1 &= \\
2(-1) - 2(-1) +1 &= 1 \\
\end{split}
\end{equation}  

Which indicates the preceptron calssifies the point (-1,-1) correctly as a positive instance

This means $\omega_0 = 1;\omega_1=2;\omega_2=-2$ for $h_2$


Now we need to create a perceptron for function g. The domain of g as a function of $h_1$ and $h_2$ can be shown as follows

\begin{center}
\begin{tikzpicture}[scale=3.0]
%\draw [help lines] (-3,-3) grid (3,3);
% Euclidean
\draw [<->](0,-1.1)--(0,1.1) node[right]{$h_2$};
\draw [<->](-1.1,0)--(1.1,0) node[right]{$h_1$};
 
\draw[red,thick,dashed] (1.1,-0.1) -- (-0.1,1.1);

%labels
\foreach \x in {-1,1}
     \draw (\x,1pt) -- (\x,-3pt) node[anchor=north] {$\x$};

\foreach \y/\ytext in {-1,1}
     \draw (1pt,\y) -- (-3pt,\y) node[anchor=east] {$\y$};
 
\draw (-1,1) node[cross,red]{};
\draw (1,-1) node[cross,red]{};
\draw (1,1) node[cross,green]{}; 

\end{tikzpicture}
\end{center}

From the above diagram we can see that a hyperplane through the points (0,1) and (1,0) will classify this perceptron

From applying equation \ref{weightline} to point $(h_1,h_2) = (1,0)$

\begin{equation}
\label{2bc1}
\begin{split}
\omega_1 x_1 + \omega_2 x_2 + \omega_0 &= 0 \\
\omega_1(1) + \omega_2(0) + \omega_0 &= 0 \\
\omega_1 &= -\omega_0\\
\end{split}
\end{equation}

and for point $(h_1,h_2) = (0,1)$

\begin{equation}
\label{2bc2}
\begin{split}
\omega_1 x_1 + \omega_2 x_2 + \omega_0 &= 0 \\
\omega_1(0) + \omega_2(1) + \omega_0 &= 0 \\
\omega_2 &= -\omega_0\\
\end{split}
\end{equation}

Now equation equations \ref{2bc1} and \ref{2bc2} we get $\omega_1 = \omega_2$

If we let $\omega_0 = -1$ then from substitution into equations \ref{2bc1} and \ref{2bc2} we get $\omega_1=\omega_2=1$.

Testing for polarity we have positive instance (1,1) 

\begin{equation}
\begin{split}
\omega_1 x_1 + \omega_2 x_2 + \omega_0 &= \\
x_1 + x_2 -1 &= \\
(1) + (1) - 1 &= 1
\end{split}
\end{equation}

Which is positive as expected

Figure \ref{fig} shows the whole network where $\omega_{1i}$ denotes the weights for the perceptron modeling $h_1$, $\omega_{2i}$ denotes the weights for the perceptron of $h_2$ and $\omega_{3i}$ denotes the weights of the final perceptron. 
\begin{figure}
\begin{tikzpicture}[scale=0.8]

    %input x
    \node[input] (x1) at (-3,6){$x_1$};    
    \node[input] (x2) at (-3,4){$x_2$}; 

	% inputs of h1
    \foreach \h [count=\hi ] in {$x_2$,$x_1$,$1$}{
    	\node[input] (f1\hi) at (0,\hi*2cm +4cm) {\h};
    }
    % inputs of h2
    \foreach \h [count=\hi ] in {$x_2$,$x_1$,$1$}{
    	\node[input] (f2\hi) at (0,\hi*2cm -2cm) {\h};
    }
    
    \draw[dotted] (x1) -- (f12);    
    \draw[dotted] (x1) -- (f22);  
    \draw[dotted] (x2) -- (f11);    
    \draw[dotted] (x2) -- (f21);  
    
	% sum h1
	\node[functions] (sumh1) at (5,8) {$\sum$};
    % sum h2
	\node[functions] (sumh2) at (5,2) {$\sum$};
	
    \node[functions] (steph1) at (6.5,8) {};
       \begin{scope}[xshift=6.5cm,yshift=8cm,scale=.75]
         \addsymbol
       \end{scope}	
       
    \node[functions] (steph2) at (6.5,2) {};
       \begin{scope}[xshift=6.5cm,yshift=2cm,scale=.75]
         \addsymbol
       \end{scope}	
	
    % weights of h1
	\foreach \h [count=\hi ] in {$w_{12} =2$,$w_{11} = -2$,$w_{10} = -1$}{%
      	\path (f1\hi) -- node[weights] (w\hi) {\h} (sumh1);
      	\draw[->] (f1\hi) -- (w\hi);
      	\draw[->] (w\hi) -- (sumh1);
    } 
    % weights of h2
	\foreach \h [count=\hi ] in {$w_{22} =-2$,$w_{21} = 2$,$w_{20} = 1$}{%
      	\path (f2\hi) -- node[weights] (w\hi) {\h} (sumh2);
      	\draw[->] (f2\hi) -- (w\hi);
      	\draw[->] (w\hi) -- (sumh2);
    } 
	
	% inputs of g
	\foreach \h [count=\hi ] in {$h_2$,$h_1$,$1$}{%
      \node[input] (h\hi) at (8,\hi*6cm-4 cm) {\h};
    }
    % sum g
	\node[functions] (sumg) at (13,6) {$\sum$};
	% weights of g
	\foreach \h [count=\hi ] in {$w_{32} =1$,$w_{31} = 1$,$w_{30} = -1$}{%
      	\path (h\hi) -- node[weights] (w\hi) {\h} (sumg);
      	\draw[->] (h\hi) -- (w\hi);
      	\draw[->] (w\hi) -- (sumg);
      	\draw[->] (sumh1) -- (steph1);
      	\draw[->] (steph1) -- (h2);
      	\draw[->] (sumh2) -- (steph2);
      	\draw[->] (steph2) -- (h1);
    }        

    \node[functions] (stepg) at (15,6) {};
       \begin{scope}[xshift=15cm,yshift=6cm,scale=.75]
         \addsymbol
       \end{scope}
    
    \draw[->] (sumg) -- (stepg);       
	\draw[->] (stepg) -- ++(1,0);
	
	% Labels
	\node[above=1cm] at (x1) {inputs};
	\node[above=1cm] at (w3) {weights};
	\node[right=1cm] at (stepg) {output};
\end{tikzpicture}
\caption{Network for question 2b}
\label{fig}
\end{figure}

\subsection{Question 2(c)}

\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
$x_1$ & $x_2$ & $\neg x_1$ & $\neg x_2$ & $x_1 \vee x_2$ & $\neg x_1 \vee \neg x_2$ & $(x_1 \vee x_2)\wedge (\neg x_1 \vee \neg x_2)$\\
\hline
-1 &-1 &1 &1 & -1& 1 & -1\\
-1 &1 &1 &-1 & 1& 1 & 1\\
1 &-1 &-1 &1 & 1& 1 & 1\\
1 &1 &-1 &-1 & 1& -1 & -1\\
\hline
\end{tabular}

Visually this is:

\begin{center}
\begin{tikzpicture}[scale=3.0]
%\draw [help lines] (-3,-3) grid (3,3);
% Euclidean
\draw [<->](0,-1.1)--(0,1.1) node[right]{$x_2$};
\draw [<->](-1.1,0)--(1.1,0) node[right]{$x_1$};
 
\draw[red,thick,dashed] (-1.0,-0.5) -- (0.5,1.0);

%labels
\foreach \x in {-1,1}
     \draw (\x,1pt) -- (\x,-3pt) node[anchor=north] {$\x$};

\foreach \y/\ytext in {-1,1}
     \draw (1pt,\y) -- (-3pt,\y) node[anchor=east] {$\y$};
 
\draw (-1,-1) node[cross,red]{};
\draw (-1,1) node[cross,green]{};
\draw (1,-1) node[cross,green]{};
\draw (1,1) node[cross,red]{}; 

\end{tikzpicture}
\end{center}



\section{Question 3}

\cite{rumel} is the original article introducing the Backpropagation algorithm. The URL for this document is in the reference.

\subsection{Question 3(a)}

We start with the following weights (Table \ref{table1}).

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
$\omega_1$ & $\omega_2$ & $\omega_3$ & $\omega_4$ & $\omega_5$ & $\omega_6$ & $\omega_7$ & $\omega_8$ & $b_1$ & $b_2$\\
\hline
0.15 &0.2 &0.25 & 0.3 & 0.4 & 0.45 & 0.5 & 0.55 & 0.35 & 0.6 \\
\hline
\end{tabular}
\caption{Start weights}\label{table1}
\end{table}

\begin{table}
\begin{tabular}{|c|c|c|c|c|}
\hline
pattern & $x_1$ & $x_2$ & $d_1$ & $d_2$ \\
\hline
$p_1$ & 0.1 & 0.1 & 0.1 & 0.9 \\
$p_2$ & 0.1 & 0.9 & 0.9 & 0.1 \\
$p_3$ & 0.9 & 0.1 & 0.9 & 0.1 \\
$p_4$ & 0.9 & 0.9 & 0.1 & 0.9 \\
\hline
\end{tabular}
\caption{Input Patterns}\label{inputPatters}
\end{table}




\subsubsection{Pattern 1}
Doing the forward pass. Starting with the values on the hidden units
\begin{equation}
\begin{split}
net_{h1} &= \omega_1 x_1 + \omega_2 x_2 + b_1 \\
         &= 0.15(0.1) + 0.2(0.1) + 0.35 \\
         &= 0.385\\
\end{split}
\end{equation}

\begin{equation}
\begin{split}
out_{h1} &= \frac{1}{1+e^{-net_{h1}}}\\ 
         &= \frac{1}{1+e^{-0.385}}\\ 
         &= 0.595\\
\end{split}
\end{equation}

\begin{equation}
\begin{split}
net_{h2} &= \omega_3 x_1 + \omega_4 x_2 + b_1 \\
         &= 0.25(0.1) + 0.3(0.1) + 0.35 \\
         &= 0.405\\
\end{split}
\end{equation}

\begin{equation}
\begin{split}
out_{h2} &= \frac{1}{1+e^{-net_{h2}}}\\ 
         &= \frac{1}{1+e^{-0.405}}\\ 
         &= 0.6\\
\end{split}
\end{equation}

Continuing with the outputs of the network

\begin{equation}
\begin{split}
net_{o1} &= \omega_5 out_{h1} + \omega_6 out_{h2} + b_2 \\
         &= 0.4(0.595) + 0.45(0.6) + 0.6 \\
         &= 1.108\\
\end{split}
\end{equation}

\begin{equation}
\begin{split}
out_{o1} &= \frac{1}{1+e^{-net_{o1}}}\\ 
         &= \frac{1}{1+e^{-1.108}}\\ 
         &= 0.752\\
\end{split}
\end{equation}

\begin{equation}
\begin{split}
net_{02} &= \omega_7 out_{h1} + \omega_8 out_{h2} + b_2 \\
         &= 0.5(0.595) + 0.55(0.6) + 0.6 \\
         &= 1.227\\
\end{split}
\end{equation}

\begin{equation}
\begin{split}
out_{o1} &= \frac{1}{1+e^{-net_{h1}}}\\ 
         &= \frac{1}{1+e^{-1.223}}\\ 
         &= 0.773\\
\end{split}
\end{equation}

Now we need to calculate the error

\begin{equation}
\begin{split}
E_{total} &= E_{o1} + E_{02} \\
          &= \frac{1}{2}(0.1-0.752)^2 + \frac{1}{2}(0.9 - 0.773)^2 \\
          &= 0.220 \\
\end{split}
\end{equation}

\begin{equation}
\begin{split}
\delta_{o1} &= -(target_{o1} -out_{o1}out_{o1}(1-out_{o1})\\
            &= -(0.1-0.752)(0.752)(1-0.752)\\
            &= 0.122\\
\end{split}
\end{equation}

\begin{equation}
\begin{split}
\pdv{E_{total}}{\omega_5} &= \delta_{o1}out_{h1}\\
                          &= 0.122(0.595)\\
                          &= 0.073\\
\end{split}
\end{equation}

We calculate the new value for $\omega_5$
\begin{equation}
\begin{split}
\omega_5^1 &= \omega_5^0 - \eta\pdv{E_{total}}{\omega_5}\\
           &= 0.4 -0.5(0.073)\\
           &= 0.364\\
\end{split}
\end{equation}

\begin{equation}
\begin{split}
\pdv{E_{total}}{\omega_6} &= \delta_{o1}out_{h2}\\
                          &= 0.122(0.6)\\
                          &= 0.073\\
\end{split}
\end{equation}

We calculate the new value for $\omega_6$
\begin{equation}
\begin{split}
\omega_6^1 &= \omega_6^0 - \eta\pdv{E_{total}}{\omega_6}\\
           &= 0.45 -0.5(0.073)\\
           &= 0.414\\
\end{split}
\end{equation}

\begin{equation}
\begin{split}
\delta_{o2} &= -(target_o2 -out_{o2}out_{o2}(1-out_{o2})\\
            &= -(0.9-0.773)(0.773)(1-0.773)\\
            &= -0.022\\
\end{split}
\end{equation}

\begin{equation}
\begin{split}
\pdv{E_{total}}{\omega_7} &= \delta_{o2}out_{h1}\\
                          &= -0.022(0.595)\\
                          &= -0.013\\
\end{split}
\end{equation}

We calculate the new value for $\omega_7$
\begin{equation}
\begin{split}
\omega_7^1 &= \omega_7^0 - \eta\pdv{E_{total}}{\omega_7}\\
           &= 0.5 -0.5(-0.013)\\
           &= 0.507\\
\end{split}
\end{equation}

\begin{equation}
\begin{split}
\pdv{E_{total}}{\omega_8} &= \delta_{o2}out_{h2}\\
                          &= -0.022(0.6)\\
                          &= -0.013\\
\end{split}
\end{equation}

We calculate the new value for $\omega_6$
\begin{equation}
\begin{split}
\omega_6^1 &= \omega_6^0 - \eta\pdv{E_{total}}{\omega_6}\\
           &= 0.55 -0.5(-0.013)\\
           &= 0.557\\
\end{split}
\end{equation}







Now we need to pass the error back to the hidden units

\begin{equation}
\begin{split}
\delta_{h1} &= (\delta_{o1}\omega_5^0 + \delta_{o2}\omega_7^0)(out_{h1})(1-out_{h1})\\
&= (0.122(0.4) -0.022(0.5))(0.595)(0.595)\\
&= 0.009\\
\end{split}
\end{equation}

\begin{equation}
\begin{split}
\pdv{E_{total}}{\omega_1} &= \delta_{h1}x_1\\ 
                          &= 0.009(0.1)\\
                          &= 0.001\\
\end{split}
\end{equation}

\begin{equation}
\begin{split}
\omega_1^1 &= \omega_1^0 - \eta\pdv{E_{total}}{\omega_1}\\
           &= 0.15 -0.5(0.001)\\
           &= 0.149\\
\end{split}
\end{equation}

\begin{equation}
\begin{split}
\pdv{E_{total}}{\omega_2} &= \delta_{h1}x_2\\ 
                          &= 0.009(0.1)\\
                          &= 0.001\\
\end{split}
\end{equation}

\begin{equation}
\begin{split}
\omega_2^1 &= \omega_2^0 - \eta\pdv{E_{total}}{\omega_2}\\
           &= 0.2 -0.5(0.001)\\
           &= 0.199\\
\end{split}
\end{equation}

\begin{equation}
\begin{split}
\delta_{h2} &= (\delta_{o1}\omega_6^0 + \delta_{o2}\omega_8^0)(out_{h2})(1-out_{h2})\\
&= (0.122(0.45) -0.022(0.55))(0.6)(0.6)\\
&= 0.010\\
\end{split}
\end{equation}

\begin{equation}
\begin{split}
\pdv{E_{total}}{\omega_3} &= \delta_{h2}x_1\\ 
                          &= 0.010(0.1)\\
                          &= 0.001\\
\end{split}
\end{equation}

\begin{equation}
\begin{split}
\omega_3^1 &= \omega_3^0 - \eta\pdv{E_{total}}{\omega_3}\\
           &= 0.25 -0.5(0.001)\\
           &= 0.249\\
\end{split}
\end{equation}

\begin{equation}
\begin{split}
\pdv{E_{total}}{\omega_4} &= \delta_{h2}x_2\\ 
                          &= 0.010(0.1)\\
                          &= 0.001\\
\end{split}
\end{equation}

\begin{equation}
\begin{split}
\omega_4^1 &= \omega_4^0 - \eta\pdv{E_{total}}{\omega_4}\\
           &= 0.3 -0.5(0.001)\\
           &= 0.299\\
\end{split}
\end{equation}

\subsubsection{Pattern 2}
\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
$\omega_1$ & $\omega_2$ & $\omega_3$ & $\omega_4$ & $\omega_5$ & $\omega_6$ & $\omega_7$ & $\omega_8$ & $b_1$ & $b_2$\\
\hline
0.149 &0.99 &0.249 & 0.299 & 0.364 & 0.414 & 0.507 & 0.557 & 0.35 & 0.6 \\
\hline
\end{tabular}
\caption{Weights after applying pattern 1}\label{tablep2}
\end{table}

Continuing with the forward pass for pattern 2
























\subsection{Question 3(b)}

According to \cite{Michell2009} the following functions can be modeled by feed forward networks. Firstly any boolean function can be represented by using 2 layers of units (not counting the input layer). But, in the worst case, the number of hidden unit will grow exponentially with the number of inputs. Secondly any Continuous bounded function can be modeled to an arbitrary level of accuracy with a network with one hidden layer. Thirdly any arbitrary function can be approximated with a network containing 2 layers of hidden units.
 
\subsection{Question 3(c)}

The error surface for multi-layered networks contains multiple local minima. The gradient descent method used can only converge to a local minima which is not necessarily the global minimum. When the gradient decent process starts close to a local minimum the algorithm will follow the gradient to the local minimum  \citep{Michell2009}.

Networks with higher dimensions may suffer less from local minima because when the network reches a local minimum in a certain dimension, the other dimentions are not necessarily at a minumum. Their weights can get the network out of some local minima  \citep{Michell2009}.
 
The problem of local minima can also lessened by using a momentum term. As in

\begin{equation}
\begin{split}
\omega_i^{j+1} &= \omega_i^j - \eta\pdv{E_{total}}{\omega_i^j} +\alpha (\omega_i^{j}-\omega_i^{j-1})
\end{split}
\end{equation}

where alpha is the momentum constant and this depends on the previous change in the weight. This way the algorithm can get out of a relatively small local minimum \citep{Michell2009}.

Another method is simulated annealing \citep{rojas};
\subsection{Question 3(d)}

A discussion of the identity function can be found in \citep[p106]{Michell2009} at $http://www.cs.ubbcluj.ro/\sim gabis/ml/ml-books/McGrawHill - Machine Learning -Tom Mitchell.pdf$

Using a 8 X 3 X 8 network we want to represent the identity function. In other words when any input node is given an high input we want the corresponding output node to be high. Thus if input 2 is high and all the other nodes are low (01000000), output 2 needs to be high and all other outputs low (01000000).  The problem is that there is not enough nodes in the hidden layer to simply forward the value from the input node to the corresponding output node. 

When we attempt to train such a network the network finds a representation in the hidden layer to represent all 8 inputs. The network uses a binary representation where the input (00100000) will be represented as (011) in the hidden units. This will then be output as (00100000). This way the network learn a new representation that it was not explicitly told.

The values from the book can be found in table \ref{table2}

\begin{table}
\begin{tabular}{|c|c|c|c|c||}
\hline
$Input$ & $h_1$ & $h_2$ & $h_3$ & $Output$ \\
\hline
10000000 & 0.89 & 0.04 & 0.08 & 10000000 \\
01000000 & 0.15 & 0.99 & 0.99 & 01000000 \\
00100000 & 0.01 & 0.97 & 0.27 & 00100000 \\
00010000 & 0.99 & 0.97 & 0.71 & 00010000 \\
00001000 & 0.03 & 0.05 & 0.02 & 00001000 \\
00000100 & 0.01 & 0.11 & 0.88 & 00000100 \\
00000010 & 0.80 & 0.01 & 0.98 & 00000010 \\
00000001 & 0.60 & 0.94 & 0.01 & 00000001 \\
\hline
\end{tabular}
\caption{Hidden unit representation}\label{table2}
\end{table}

\subsection{Question 3(e)}

The sigmoid function is defined as

\begin{equation}
\sigma(y) = \frac{1}{1+e^{-y}}
\end{equation}

\bibliography{mybib}
\end{document}
